{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2I-01BY2C4_K"
   },
   "source": [
    "##**Object recognition and computer vision 2019/2020**  \n",
    "\n",
    "Marouen Chafrouda \n",
    "\n",
    "Marouen.chafrouda@student.ecp.fr\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ebzH0ThzC4_K"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "irIf-COVFE9z"
   },
   "source": [
    "We start off by recovering the Dataset of the Challenge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "5GYRBjKxC4_M",
    "outputId": "e335000d-eb62-43a6-dc2e-980d319a8dd7"
   },
   "outputs": [],
   "source": [
    "!wget https://www.di.ens.fr/willow/teaching/recvis18/assignment3/bird_dataset.zip \n",
    "!unzip bird_dataset.zip  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PFOh7GRYFKBW"
   },
   "source": [
    "###Bounding Boxes Extraction with Faster RCNN\n",
    "Now we'll extract the bounding Boxes from the images of dataset and save them another folder while randomly splitting them to training/validation, we'll use our own split here and not the provided split as that one has unbalanced data among classes in Validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W3uYsE5EC4_O"
   },
   "outputs": [],
   "source": [
    "class BoundingBoxes(object):\n",
    "    \"\"\"return a box with just the photo of the birds ie the content of the bounding box \"\"\"\n",
    "    def __init__(self,model,device=\"cuda:0\"):\n",
    "        self.model=model\n",
    "        self.device=device\n",
    "    def __call__(self, img):\n",
    "        #bounding boxes\n",
    "        image=img.to(self.device)\n",
    "        prediction=self.model([image])[0]\n",
    "        image=image.cpu()\n",
    "        try:\n",
    "            first_bird=np.where(prediction['labels'].cpu().numpy()==16)[0][0]#16 is the class for birds \n",
    "            pred=prediction['boxes'][first_bird]\n",
    "            pred_int=pred.int()\n",
    "            img=img[:,pred_int[1]:pred_int[3],pred_int[0]:pred_int[2]]\n",
    "            return img \n",
    "        except :\n",
    "            print('no birds')\n",
    "            #if no birds are found just return the whole image\n",
    "            return img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "gKH3q8LqC4_T",
    "outputId": "ec7f1bf8-8d02-4d42-d0b7-246236d4c395"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
      "100%|██████████| 160M/160M [00:02<00:00, 56.8MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform()\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(original_name=FrozenBatchNorm2d)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign()\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Model for Bounding Box extraction : Faster RCNN\"\"\"\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nNVSlaCGC4_V"
   },
   "outputs": [],
   "source": [
    "# Loading all  Images and Creating their Bouding Boxes\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=model.to(device)\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    BoundingBoxes(model,device),\n",
    "    transforms.ToPILImage()\n",
    "]),\n",
    "    'val': transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    BoundingBoxes(model,device),\n",
    "    transforms.ToPILImage()\n",
    "]),\n",
    "}\n",
    "data='bird_dataset'\n",
    "data_dir = {'train':data + '/train_images','val':data + '/val_images'}\n",
    "image_datasets = {x: datasets.ImageFolder(data_dir[x],\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tvMaaLxtC4_X"
   },
   "outputs": [],
   "source": [
    "#Splitting between Training and validation with sklearn's train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "tr_len=len(image_datasets['train'])\n",
    "val_len=len(image_datasets['val'])\n",
    "num =tr_len +val_len\n",
    "indices = list(range(num))\n",
    "labels=image_datasets['train'].targets+image_datasets['val'].targets\n",
    "train_idx,valid_idx,_,_ = train_test_split(indices,labels,test_size=0.2,stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GL4BEBFvC4_Y"
   },
   "outputs": [],
   "source": [
    "#Saving Bounded Training Images\n",
    "for i in train_idx :\n",
    "    if i < tr_len:\n",
    "        image,label=image_datasets['train'].__getitem__(i)\n",
    "    else:\n",
    "        image,label=image_datasets['val'].__getitem__(i-tr_len)\n",
    "#Folders Here Would need to be changed Depending on if we work locally or on colab/Kaggle Kernels        \n",
    "    filename = str(label)+'/'+str(i)+'.jpg'\n",
    "    os.makedirs('/content/bird/Train/'+str(label),exist_ok=True)\n",
    "    image.save('/content/bird/Train/'+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Ra99N0EC4_a"
   },
   "outputs": [],
   "source": [
    "#Saving Bounded Validation Images\n",
    "for i in valid_idx :\n",
    "    if i < tr_len:\n",
    "        image,label=image_datasets['train'].__getitem__(i)\n",
    "    else:\n",
    "        image,label=image_datasets['val'].__getitem__(i-tr_len)\n",
    "#Folders Here Would need to be changed Depending on if we work locally or on colab/Kaggle Kernels        \n",
    "    filename = str(label)+'/'+str(i)+'.jpg'\n",
    "    os.makedirs('/content/bird/Validation/'+str(label),exist_ok=True)\n",
    "    image.save('/content/bird/Validation/'+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YMIBaiBUC4_b"
   },
   "outputs": [],
   "source": [
    "#Liberating some GPU memory Here\n",
    "model=model.cpu()\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YMNcdmwzG51M"
   },
   "source": [
    "### Classification with Transfer Learning : Resnet-101\n",
    "We'll use the feature extraction part of the pretrained Resnet model available in pytorch's Torchvision module, the NN is trained on Imagenet  dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X8tbwaz9H7Pb"
   },
   "outputs": [],
   "source": [
    "model_conv = torchvision.models.resnet101(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False #we freeze the weights of the part responsible for Feature extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bBs5xxYMC4_Q"
   },
   "outputs": [],
   "source": [
    "#torch.multiprocessing.set_start_method('spawn',force=True)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "nclasses=20\n",
    "\n",
    "model_conv.fc= nn.Sequential(nn.Linear(in_features=num_ftrs, out_features=num_ftrs),\n",
    "                              nn.ReLU(inplace=True),\n",
    "                              nn.Dropout(.5),\n",
    "                              nn.Linear(in_features=num_ftrs, out_features=num_ftrs),\n",
    "                              nn.ReLU(inplace=True),\n",
    "                              nn.Dropout(.5),\n",
    "                              nn.Linear(in_features=num_ftrs, out_features=nclasses))#nn.Linear(in_features=num_ftrs, out_features=nclasses)\n",
    "\"\"\"\n",
    "model_conv.classifier[6]= nn.Sequential(nn.Linear(in_features=num_ftrs, out_features=num_ftrs),\n",
    "                              nn.ReLU(inplace=True),\n",
    "                              nn.Dropout(.5),\n",
    "                              nn.Linear(in_features=num_ftrs, out_features=num_ftrs),\n",
    "                              nn.ReLU(inplace=True),\n",
    "                              nn.Dropout(.5),\n",
    "                              nn.Linear(in_features=num_ftrs, out_features=nclasses))#nn.Linear(in_features=num_ftrs, out_features=nclasses)\n",
    "\"\"\"\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opposed to before.\n",
    "optimizer_conv = optim.Adam(model_conv.fc.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lLw-3qZJEy3E"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, dataloaders,num_epochs=25):\n",
    "    since = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_loss=10\n",
    "    losses={'train':[],'val':[]}\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() *100 / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            losses[phase]+=[epoch_loss]\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc >= best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model,losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UAp8Skz_Hazs"
   },
   "source": [
    "###Data Augmentation\n",
    "Now before Starting the training of our NN we'll do some data augmentation, for doing this we'll use Imgaug, a very Handy library that provides a wide range of possible image augmentation techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "colab_type": "code",
    "id": "ZExOzTKuC4_d",
    "outputId": "b6431eac-1980-4ddc-92d7-bf2321cdbb0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/aleju/imgaug\n",
      "  Cloning https://github.com/aleju/imgaug to /tmp/pip-req-build-mjx40951\n",
      "  Running command git clone -q https://github.com/aleju/imgaug /tmp/pip-req-build-mjx40951\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from imgaug==0.3.0) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from imgaug==0.3.0) (1.17.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from imgaug==0.3.0) (1.3.2)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from imgaug==0.3.0) (4.3.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from imgaug==0.3.0) (3.1.1)\n",
      "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.6/dist-packages (from imgaug==0.3.0) (0.15.0)\n",
      "Collecting opencv-python-headless\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/96/b2824df85d8c5f148125e4fee073a62fde17639502fe67042b212ebfe488/opencv_python_headless-4.1.2.30-cp36-cp36m-manylinux1_x86_64.whl (21.8MB)\n",
      "\u001b[K     |████████████████████████████████| 21.8MB 422kB/s \n",
      "\u001b[?25hRequirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (from imgaug==0.3.0) (2.4.1)\n",
      "Requirement already satisfied: Shapely in /usr/local/lib/python3.6/dist-packages (from imgaug==0.3.0) (1.6.4.post2)\n",
      "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->imgaug==0.3.0) (0.46)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug==0.3.0) (2.4.5)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug==0.3.0) (2.6.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug==0.3.0) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug==0.3.0) (1.1.0)\n",
      "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.14.2->imgaug==0.3.0) (2.4)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.14.2->imgaug==0.3.0) (1.1.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->imgaug==0.3.0) (41.6.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.14.2->imgaug==0.3.0) (4.4.1)\n",
      "Building wheels for collected packages: imgaug\n",
      "  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for imgaug: filename=imgaug-0.3.0-cp36-none-any.whl size=891612 sha256=e8c5419b36adde7d1569aa5de8af6331c915a6ed87e0f8aa4d38a337927cd981\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-xtwk6erh/wheels/9c/f6/aa/41dcf2f29cc1de1da4ad840ef5393514bead64ac9e644260ff\n",
      "Successfully built imgaug\n",
      "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.3.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: opencv-python-headless, imgaug\n",
      "  Found existing installation: imgaug 0.2.9\n",
      "    Uninstalling imgaug-0.2.9:\n",
      "      Successfully uninstalled imgaug-0.2.9\n",
      "Successfully installed imgaug-0.3.0 opencv-python-headless-4.1.2.30\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Imgaug will be used for data augmentation\"\"\"\n",
    "!pip install git+https://github.com/aleju/imgaug\n",
    "from imgaug import augmenters as iaa\n",
    "import imgaug as ia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RGmltseFC4_e"
   },
   "outputs": [],
   "source": [
    "class ImgAugTransform:\n",
    "  def __init__(self):\n",
    "    self.aug =iaa.Sequential([\n",
    "    iaa.Sometimes(0.5, iaa.GaussianBlur((0, 1.0))), # apply Gaussian blur with a sigma between 0 and 3 to 25% of the images\n",
    "    # apply one of the augmentations: Dropout or CoarseDropout to 50% of images\n",
    "    iaa.Sometimes(0.5,iaa.OneOf([\n",
    "        iaa.Dropout((0.01, 0.1), per_channel=0.5), # randomly remove up to 10% of the pixels\n",
    "        iaa.CoarseDropout((0.03, 0.15), size_percent=(0.02, 0.25), per_channel=0.2),\n",
    "    ])),\n",
    "    # apply from 0 to 3 of the augmentations from the list to 50% of images\n",
    "    iaa.Sometimes(0.5,iaa.SomeOf((0, 3),[\n",
    "        iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 0.5)), # sharpen images\n",
    "        iaa.Emboss(alpha=(0, 1.0), strength=(0, 1.0)), # emboss images\n",
    "        iaa.Fliplr(1.0) # horizontally flip\n",
    "        \n",
    "    ]))\n",
    "],\n",
    "random_order=True # apply the augmentations in random order\n",
    ")\n",
    "  def __call__(self, img):\n",
    "    img = np.array(img)\n",
    "    return self.aug.augment_image(img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kroSoUGEC4_g"
   },
   "outputs": [],
   "source": [
    "# Loading Bounded images now \n",
    "data_transforms = {\n",
    "    'train': torchvision.transforms.Compose([\n",
    "    #torchvision.transforms.Resize((256, 256)),\n",
    "    ImgAugTransform(),\n",
    "    lambda x: PIL.Image.fromarray(x),\n",
    "    torchvision.transforms.RandomRotation(degrees=15,expand=True),\n",
    "    torchvision.transforms.Resize((256, 256)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "]),\n",
    "    'val': torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((256, 256)),\n",
    "    #transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "]),\n",
    "}\n",
    "data='bird'\n",
    "data_dir = {'train':data + '/Train','val':data + '/Validation'}\n",
    "image_datasets = {x: datasets.ImageFolder(data_dir[x],\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "shuffle={'train':True ,'val':False}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=32,\n",
    "                                             shuffle=shuffle[x], num_workers=0)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xbTtu6U8Jioc"
   },
   "source": [
    "###Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "colab_type": "code",
    "id": "fihsbM6YC4_k",
    "outputId": "1ff61ece-50a5-4df1-9c57-cf1aac34f94c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/6\n",
      "----------\n",
      "train Loss: 3.1031 Acc: 4.5359\n",
      "val Loss: 2.8203 Acc: 22.3629\n",
      "\n",
      "Epoch 1/6\n",
      "----------\n",
      "train Loss: 2.7320 Acc: 15.8228\n",
      "val Loss: 2.3043 Acc: 27.4262\n",
      "\n",
      "Epoch 2/6\n",
      "----------\n",
      "train Loss: 2.3160 Acc: 25.3165\n",
      "val Loss: 1.7814 Acc: 43.0380\n",
      "\n",
      "Epoch 3/6\n",
      "----------\n",
      "train Loss: 2.0245 Acc: 33.5443\n",
      "val Loss: 1.3982 Acc: 61.6034\n",
      "\n",
      "Epoch 4/6\n",
      "----------\n",
      "train Loss: 1.8224 Acc: 39.8734\n",
      "val Loss: 1.4099 Acc: 52.7426\n",
      "\n",
      "Epoch 5/6\n",
      "----------\n",
      "train Loss: 1.7565 Acc: 44.1983\n",
      "val Loss: 1.2089 Acc: 60.3376\n",
      "\n",
      "Epoch 6/6\n",
      "----------\n",
      "train Loss: 1.6926 Acc: 45.9916\n",
      "val Loss: 1.2444 Acc: 56.1181\n",
      "\n",
      "Training complete in 2m 54s\n",
      "Best val Acc: 61.603376\n"
     ]
    }
   ],
   "source": [
    "model_conv,Losses = train_model(model_conv, criterion, optimizer_conv,\n",
    "                         exp_lr_scheduler,dataloaders, num_epochs=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OUAL30BiIa-7"
   },
   "source": [
    "The fully connected layers we added to the Model are trained for a few Epochs and then we unfreeze the weights of the whole network and let it train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "gFCkznJ7C4_l",
    "outputId": "fabe22cb-952a-4736-f61e-5af2bd4a8839"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/18\n",
      "----------\n",
      "train Loss: 1.3674 Acc: 55.2743\n",
      "val Loss: 0.7097 Acc: 76.7932\n",
      "\n",
      "Epoch 1/18\n",
      "----------\n",
      "train Loss: 0.9533 Acc: 68.5654\n",
      "val Loss: 0.6029 Acc: 83.9662\n",
      "\n",
      "Epoch 2/18\n",
      "----------\n",
      "train Loss: 0.6984 Acc: 77.6371\n",
      "val Loss: 0.3944 Acc: 87.3418\n",
      "\n",
      "Epoch 3/18\n",
      "----------\n",
      "train Loss: 0.6462 Acc: 77.6371\n",
      "val Loss: 0.4470 Acc: 84.3882\n",
      "\n",
      "Epoch 4/18\n",
      "----------\n",
      "train Loss: 0.6097 Acc: 79.3249\n",
      "val Loss: 0.4020 Acc: 86.9198\n",
      "\n",
      "Epoch 5/18\n",
      "----------\n",
      "train Loss: 0.4656 Acc: 84.0717\n",
      "val Loss: 0.4360 Acc: 85.2321\n",
      "\n",
      "Epoch 6/18\n",
      "----------\n",
      "train Loss: 0.4210 Acc: 86.2869\n",
      "val Loss: 0.4623 Acc: 84.8101\n",
      "\n",
      "Epoch 7/18\n",
      "----------\n",
      "train Loss: 0.3496 Acc: 88.2911\n",
      "val Loss: 0.4219 Acc: 86.9198\n",
      "\n",
      "Epoch 8/18\n",
      "----------\n",
      "train Loss: 0.3132 Acc: 89.9789\n",
      "val Loss: 0.3306 Acc: 87.7637\n",
      "\n",
      "Epoch 9/18\n",
      "----------\n",
      "train Loss: 0.2671 Acc: 92.2996\n",
      "val Loss: 0.3241 Acc: 89.8734\n",
      "\n",
      "Epoch 10/18\n",
      "----------\n",
      "train Loss: 0.2371 Acc: 92.0886\n",
      "val Loss: 0.3607 Acc: 88.6076\n",
      "\n",
      "Epoch 11/18\n",
      "----------\n",
      "train Loss: 0.2334 Acc: 93.4599\n",
      "val Loss: 0.3514 Acc: 89.0295\n",
      "\n",
      "Epoch 12/18\n",
      "----------\n",
      "train Loss: 0.1977 Acc: 94.4093\n",
      "val Loss: 0.3208 Acc: 89.0295\n",
      "\n",
      "Epoch 13/18\n",
      "----------\n",
      "train Loss: 0.1976 Acc: 92.8270\n",
      "val Loss: 0.3285 Acc: 89.8734\n",
      "\n",
      "Epoch 14/18\n",
      "----------\n",
      "train Loss: 0.1930 Acc: 94.9367\n",
      "val Loss: 0.3078 Acc: 89.4515\n",
      "\n",
      "Epoch 15/18\n",
      "----------\n",
      "train Loss: 0.2164 Acc: 94.0928\n",
      "val Loss: 0.3107 Acc: 89.4515\n",
      "\n",
      "Epoch 16/18\n",
      "----------\n",
      "train Loss: 0.2108 Acc: 93.3544\n",
      "val Loss: 0.3217 Acc: 89.8734\n",
      "\n",
      "Epoch 17/18\n",
      "----------\n",
      "train Loss: 0.2033 Acc: 93.1435\n",
      "val Loss: 0.3209 Acc: 89.8734\n",
      "\n",
      "Epoch 18/18\n",
      "----------\n",
      "train Loss: 0.1977 Acc: 93.7764\n",
      "val Loss: 0.3110 Acc: 89.4515\n",
      "\n",
      "Training complete in 16m 39s\n",
      "Best val Acc: 89.873418\n"
     ]
    }
   ],
   "source": [
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = True\n",
    "optimizer_conv = optim.Adam(model_conv.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n",
    "torch.cuda.empty_cache()\n",
    "model_conv,L = train_model(model_conv, criterion, optimizer_conv,\n",
    "                         exp_lr_scheduler,dataloaders, num_epochs=19)\n",
    "Losses['train']+=L['train']\n",
    "Losses['val']+=L['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "rWtSvcejMh08",
    "outputId": "baa4cea2-7e22-4407-90f0-86dccd72dcd2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f820a4b76a0>"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSKPWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(Losses['train'], label='train')\n",
    "plt.plot(Losses['val'],label='val')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "42oPn5iJSupg"
   },
   "source": [
    "This  gets us around 90 % accuracy on the Validation set the we created from 20% of all the available data and on The test set on kaggle we get 0.80 Test score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "msk6cj6PJfBn"
   },
   "source": [
    "###Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y4HijyljKPBL"
   },
   "source": [
    "We create a custom class so we can track images and their names and still use pytorch's ImageFolder class to load the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "1jh7GuL4C4_r",
    "outputId": "949b6596-d11d-4c59-b54e-29ca78381773"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no birds\n"
     ]
    }
   ],
   "source": [
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths. Extends\n",
    "    torchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "\n",
    "    # override the __getitem__ method. this is the method that dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path\n",
    "\n",
    "\"\"\"Model for Bounding Box extraction : Faster RCNN\"\"\"\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "mode=model.to(device)\n",
    "test_transforms=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    BoundingBoxes(model,device),\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "test_loader = torch.utils.data.DataLoader(ImageFolderWithPaths('bird_dataset' + '/test_images',\n",
    "                         transform=test_transforms), shuffle=False, num_workers=0) # our custom dataset\n",
    "\n",
    "\n",
    "# iterate over data\n",
    "L=[['Id','Category']]\n",
    "with torch.no_grad():\n",
    "    for inputs, _, paths in test_loader:\n",
    "        # use the above variables freely\n",
    "        inputs = inputs.to(device)\n",
    "        output = model_conv(inputs)\n",
    "        _, pred = torch.max(output, 1)\n",
    "        L+=[[paths[0].split('/')[3][:-4],pred.item()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wl5hNH_LC4_t"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(L).to_csv('recvis_kag1.csv')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Transfer_learning_recvis (3).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
