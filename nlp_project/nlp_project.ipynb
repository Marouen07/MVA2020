{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "nlp_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOUNl46UDPfl",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning for NLP - Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k12wLsW5DPfn",
        "colab_type": "text"
      },
      "source": [
        "RULES:\n",
        "\n",
        "* Do not create any additional cell\n",
        "\n",
        "* Fill in the blanks\n",
        "\n",
        "* All cells should be runnable (modulo trivial compatibility bugs that we'd fix)\n",
        "\n",
        "* 4 / 20 points will be allocated to the clarity of your code\n",
        "\n",
        "* Efficient code will have a bonus\n",
        "\n",
        "DELIVERABLE:\n",
        "\n",
        "* the pdf with your answers\n",
        "* this notebook\n",
        "* the predictions of the SST test set\n",
        "\n",
        "DO NOT INCLUDE THE DATASETS IN THE DELIVERABLE.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "zn-FaobMDPfo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Python 3.6 or above is required\n",
        "from collections import defaultdict\n",
        "import gzip\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from urllib.request import urlretrieve\n",
        "import re \n",
        "from collections import defaultdict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzbsVQG0DPfr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "try: \n",
        "  os.mkdir('./data')\n",
        "except  OSError :\n",
        "  pass\n",
        "PATH_TO_DATA = Path('data/')\n",
        "# Download word vectors, might take a few minutes and about ~3GB of storage space\n",
        "en_embeddings_path = PATH_TO_DATA / 'cc.en.300.vec.gz'\n",
        "if not en_embeddings_path.exists():\n",
        "    urlretrieve('https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz', en_embeddings_path)\n",
        "fr_embeddings_path = PATH_TO_DATA / 'cc.fr.300.vec.gz'\n",
        "if not fr_embeddings_path.exists():\n",
        "    urlretrieve('https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.vec.gz', fr_embeddings_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQlPRCa1DPfu",
        "colab_type": "text"
      },
      "source": [
        "# 1) Monolingual (English) word embeddings "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKciiSt5DPfv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Word2Vec():\n",
        "    def __init__(self, filepath, vocab_size=50000):\n",
        "        self.words, self.embeddings = self.load_wordvec(filepath, vocab_size)\n",
        "        # Mappings for O(1) retrieval:\n",
        "        self.word2id = {word: idx for idx, word in enumerate(self.words)}\n",
        "        self.id2word = {idx: word for idx, word in enumerate(self.words)}\n",
        "    \n",
        "    def load_wordvec(self, filepath, vocab_size):\n",
        "        assert str(filepath).endswith('.gz')\n",
        "        words = []\n",
        "        embeddings = []\n",
        "        with gzip.open(filepath, 'rt',encoding=\"utf8\") as f:  # Read compressed file directly\n",
        "            next(f)  # Skip header\n",
        "            for i, line in enumerate(f):\n",
        "                word, vec = line.split(' ', 1)\n",
        "                words.append(word)\n",
        "                embeddings.append(np.fromstring(vec, sep=' '))\n",
        "                if i == (vocab_size - 1):\n",
        "                    break\n",
        "        print('Loaded %s pretrained word vectors' % (len(words)))\n",
        "        return words, np.vstack(embeddings)\n",
        "    \n",
        "    def encode(self, word):\n",
        "        \n",
        "        # Returns the 1D embedding of a given word\n",
        "        try:\n",
        "            return self.embeddings[self.word2id[word]]\n",
        "        except KeyError:\n",
        "            #print(\"word \",word, \" not in vocab\")\n",
        "            return np.zeros(self.embeddings[0].shape)\n",
        "        \n",
        "    \n",
        "    def score(self, word1, word2):\n",
        "        # Return the cosine similarity: use np.dot & np.linalg.norm\n",
        "        v1=self.encode(word1)\n",
        "        v2=self.encode(word2)\n",
        "        return (v1.T@v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))\n",
        "    \n",
        "    def most_similar(self, word, k=5):\n",
        "        # Returns the k most similar words: self.score & np.argsort\n",
        "        scores=[self.score(word,vocab) for vocab in self.words]\n",
        "        sorted_scores=np.flip(np.argsort(scores,axis=None))\n",
        "        return [self.id2word[idx] for idx in sorted_scores[1:k+1]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "qAFU2OgLDPfz",
        "colab_type": "code",
        "outputId": "46594802-d9d5-419c-a0aa-a2929c1cce55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        }
      },
      "source": [
        "word2vec = Word2Vec(en_embeddings_path, vocab_size=50000)\n",
        "# You will be evaluated on the output of the following:\n",
        "for word1, word2 in zip(('cat', 'cat', 'cat', 'Paris', 'Paris', 'Paris', 'Paris'), ('tree', 'dog', 'pet', 'France', 'Germany', 'baguette', 'donut')):\n",
        "    print(word1, word2, word2vec.score(word1, word2))\n",
        "for word in ['cat', 'dog', 'dogs', 'Paris', 'Germany']:\n",
        "    print(word2vec.most_similar(word))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 50000 pretrained word vectors\n",
            "cat tree 0.2644975466165475\n",
            "cat dog 0.7078641298542562\n",
            "cat pet 0.6753313359976381\n",
            "Paris France 0.6892958925806542\n",
            "Paris Germany 0.4051242286737548\n",
            "Paris baguette 0.29399958277802224\n",
            "Paris donut -0.006588507552348005\n",
            "['cats', 'kitty', 'kitten', 'feline', 'dog']\n",
            "['dogs', 'puppy', 'pup', 'canine', 'pet']\n",
            "['dog', 'cats', 'puppies', 'Dogs', 'pets']\n",
            "['France', 'Parisian', 'Marseille', 'Brussels', 'Strasbourg']\n",
            "['Austria', 'Europe', 'Berlin', 'Hamburg', 'Bavaria']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "VN-bDFQEDPf3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BagOfWords():\n",
        "    \n",
        "    def __init__(self, word2vec):\n",
        "        self.word2vec = word2vec\n",
        "    \n",
        "    def build_idf(self, sentences):\n",
        "        # build the idf dictionary: associate each word to its idf value\n",
        "        # -> idf = {word: idf_value, ...}\n",
        "        occ=defaultdict(float)\n",
        "        for sentence in sentences:\n",
        "            words=np.unique([word for word in re.sub(r'\\W+', ' ', sentence).split()])\n",
        "            for word in words:\n",
        "                occ[word]+=1\n",
        "        D=len(sentences)\n",
        "        idf={k:np.log(D/v) for k,v in occ.items()}\n",
        "        return idf\n",
        "    \n",
        "    def encode(self, sentence, idf=None):\n",
        "        # Takes a sentence as input, returns the sentence embedding\n",
        "        # mean of word vectors\n",
        "        words=[word for word in re.sub(r'\\W+', ' ', sentence).split()]\n",
        "        if idf is None:\n",
        "            words=[self.word2vec.encode(word) for word in words]\n",
        "            return np.average(words,axis=0)\n",
        "        else:\n",
        "            # idf-weighted mean of word vectors\n",
        "            words=[self.word2vec.encode(word)*idf[word] for word in words]\n",
        "            return np.average(words,axis=0)\n",
        "\n",
        "    def score(self, sentence1, sentence2, idf=None):\n",
        "        # Return the cosine similarity: use np.dot & np.linalg.norm\n",
        "        v1=self.encode(sentence1,idf)\n",
        "        v2=self.encode(sentence2,idf)\n",
        "        return (v1.T@v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))\n",
        "    def most_similar(self, sentence, sentences, idf=None, k=5):\n",
        "        # Return most similar sentences\n",
        "        query = self.encode(sentence, idf)\n",
        "        keys = np.vstack([self.encode(sentence, idf) for sentence in sentences])\n",
        "        scores=[self.score(sentence,vocab,idf) for vocab in sentences]\n",
        "        sorted_scores=np.flip(np.argsort(scores,axis=None))\n",
        "        return [sentences[idx] for idx in sorted_scores[1:k+1]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "scrolled": false,
        "id": "95Du0QI_DPf6",
        "colab_type": "code",
        "outputId": "8096985b-99af-4a8c-aa1b-ae242b291091",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        }
      },
      "source": [
        "word2vec = Word2Vec(en_embeddings_path, vocab_size=2000000)\n",
        "sentence2vec = BagOfWords(word2vec)\n",
        "\n",
        "# Load sentences in \"PATH_TO_DATA/sentences.txt\"\n",
        "filepath = PATH_TO_DATA / 'sentences.txt'\n",
        "with open(filepath, 'r') as f:\n",
        "    sentences = [line.strip('\\n') for line in f]\n",
        "\n",
        "\n",
        "# You will be evaluated on the output of the following:\n",
        "print('\\n\\tAverage of word embeddings')\n",
        "sentence1 = sentences[7]\n",
        "sentence2 = sentences[13]\n",
        "print(sentence1)\n",
        "print(sentence2)\n",
        "print(sentence2vec.score(sentence1, sentence2))\n",
        "sentence = sentences[10]\n",
        "similar_sentences = sentence2vec.most_similar(sentence, sentences)  # BagOfWords-mean\n",
        "print(sentence)\n",
        "\n",
        "for i, sentence in enumerate(similar_sentences):\n",
        "    print(str(i+1) + ')', sentence)\n",
        "    \n",
        "# Build idf scores for each word\n",
        "idf = sentence2vec.build_idf(sentences)\n",
        "\n",
        "\n",
        "print('\\n\\tidf weighted average of word embeddings')\n",
        "print(sentence1)\n",
        "print(sentence2)\n",
        "print(sentence2vec.score(sentence1, sentence2, idf))\n",
        "sentence = sentences[10]\n",
        "similar_sentences = sentence2vec.most_similar(sentence, sentences, idf)  # BagOfWords-idf\n",
        "print(sentence)\n",
        "for i, sentence in enumerate(similar_sentences):\n",
        "    print(str(i+1) + ')', sentence)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 2000000 pretrained word vectors\n",
            "\n",
            "\tAverage of word embeddings\n",
            "1 man singing and 1 man playing a saxophone in a concert . \n",
            "10 people venture out to go crosscountry skiing . \n",
            "0.6405950330419073\n",
            "1 smiling african american boy . \n",
            "1) 2 woman dancing while pointing . \n",
            "2) 5 women and 1 man are smiling for the camera . \n",
            "3) 3 males and 1 woman enjoying a sporting event \n",
            "4) 2 chinese people wearing traditional clothes \n",
            "5) a young boy and 2 girls open christmas presents . \n",
            "\n",
            "\tidf weighted average of word embeddings\n",
            "1 man singing and 1 man playing a saxophone in a concert . \n",
            "10 people venture out to go crosscountry skiing . \n",
            "0.6369156540016581\n",
            "1 smiling african american boy . \n",
            "1) 1 man singing and 1 man playing a saxophone in a concert . \n",
            "2) two women and 1 man walking across the street . \n",
            "3) 2 guys facing away from camera , 1 girl smiling at camera with blue shirt , 1 guy with a beverage with a jacket on . \n",
            "4) 3 males and 1 woman enjoying a sporting event \n",
            "5) 5 women and 1 man are smiling for the camera . \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x1KXBM-DPf9",
        "colab_type": "text"
      },
      "source": [
        "# 2) Multilingual (English-French) word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebnuVzBMDPf-",
        "colab_type": "text"
      },
      "source": [
        "Let's consider a bilingual dictionary of size V_a (e.g French-English).\n",
        "\n",
        "Let's define **X** and **Y** the **French** and **English** matrices.\n",
        "\n",
        "They contain the embeddings associated to the words in the bilingual dictionary.\n",
        "\n",
        "We want to find a **mapping W** that will project the source word space (e.g French) to the target word space (e.g English).\n",
        "\n",
        "Procrustes : **W\\* = argmin || W.X - Y ||  s.t  W^T.W = Id**\n",
        "has a closed form solution:\n",
        "**W = U.V^T  where  U.Sig.V^T = SVD(Y.X^T)**\n",
        "\n",
        "In what follows, you are asked to: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4bpJpG8DPf_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultilingualWordAligner:\n",
        "    \n",
        "    def __init__(self, fr_word2vec, en_word2vec):\n",
        "        self.fr_word2vec = fr_word2vec\n",
        "        self.en_word2vec = en_word2vec\n",
        "        self.aligned_fr_embeddings = self.get_aligned_fr_embeddings()\n",
        "        \n",
        "    def get_aligned_fr_embeddings(self):\n",
        "        # 1 - Get words that appear in both vocabs (= identical character strings)\n",
        "        identical=[]\n",
        "        for word in self.fr_word2vec.words:\n",
        "            if word in self.en_word2vec.word2id: #this is in O(1) in a dict \n",
        "                identical+=[word]\n",
        "        X,Y=[],[]\n",
        "        for word in identical:\n",
        "            X+=[self.fr_word2vec.encode(word)]\n",
        "            Y+=[self.en_word2vec.encode(word)]\n",
        "        X=np.vstack(X).T\n",
        "        Y=np.vstack(Y).T\n",
        "        #     Use it to create the matrix X (emb_dim, vocab_size) and Y (emb_dim, vocab_size) (of embeddings for these words)\n",
        "        assert X.shape[0] == 300 and Y.shape[0] == 300\n",
        "        \n",
        "        # 2 - Solve the Procrustes using the numpy package and: np.linalg.svd() and get the optimal W\n",
        "        U,_,Vh=np.linalg.svd(Y@X.T)\n",
        "        W=U@Vh\n",
        "        #     Now self.fr_word2vec.embeddings * W.transpose() is in the same space as en_word2vec.embeddings\n",
        "        assert W.shape == (300, 300)\n",
        "        return np.matmul(fr_word2vec.embeddings, W.transpose())\n",
        "        \n",
        "    def get_closest_english_words(self, fr_word, k=3):\n",
        "        # 3 - Return the top k English nearest neighbors to the input French word\n",
        "        en_embed=self.aligned_fr_embeddings[self.fr_word2vec.word2id[fr_word]]\n",
        "        #we'll add the French Word to The English Vocab along with its \"english embedding\"\n",
        "        vocab_size=len(self.en_word2vec.id2word)\n",
        "        self.en_word2vec.embeddings=np.vstack((self.en_word2vec.embeddings,en_embed))\n",
        "        self.en_word2vec.word2id[fr_word]=vocab_size\n",
        "        self.en_word2vec.id2word[vocab_size]=fr_word # although we won't need this for the closest words\n",
        "        return self.en_word2vec.most_similar(fr_word,k)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rC5hJe0DPgD",
        "colab_type": "code",
        "outputId": "81361ebe-051f-42e0-aa29-4d0523abe36a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        }
      },
      "source": [
        "fr_word2vec = Word2Vec(fr_embeddings_path, vocab_size=50000)\n",
        "en_word2vec = Word2Vec(en_embeddings_path, vocab_size=50000)\n",
        "multilingual_word_aligner = MultilingualWordAligner(fr_word2vec, en_word2vec)\n",
        "\n",
        "# You will be evaluated on the output of the following:\n",
        "fr_words = ['chat', 'chien', 'voiture', 'zut']\n",
        "k = 3\n",
        "for fr_word in fr_words:\n",
        "    print('-' * 10)\n",
        "    print(f'fr: \"{fr_word}\"')\n",
        "    en_words = multilingual_word_aligner.get_closest_english_words(fr_word, k=3)\n",
        "    for en_word in en_words:\n",
        "        print(f'en: \"{en_word}\"')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 50000 pretrained word vectors\n",
            "Loaded 50000 pretrained word vectors\n",
            "----------\n",
            "fr: \"chat\"\n",
            "en: \"cat\"\n",
            "en: \"kitten\"\n",
            "en: \"kitty\"\n",
            "----------\n",
            "fr: \"chien\"\n",
            "en: \"dog\"\n",
            "en: \"cat\"\n",
            "en: \"pet\"\n",
            "----------\n",
            "fr: \"voiture\"\n",
            "en: \"vehicle\"\n",
            "en: \"automobile\"\n",
            "en: \"motorbike\"\n",
            "----------\n",
            "fr: \"zut\"\n",
            "en: \"Ah\"\n",
            "en: \"ah\"\n",
            "en: \"nope\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4hsOAD4DPgG",
        "colab_type": "text"
      },
      "source": [
        "If you want to dive deeper on this subject: https://github.com/facebookresearch/MUSE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adGKPU82DPgH",
        "colab_type": "text"
      },
      "source": [
        "# 3) Sentence classification with BoV and scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndBh4fvpDPgI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1 - Load train/dev/test of Stanford Sentiment TreeBank (SST)\n",
        "#     (https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n",
        "train_filepath = PATH_TO_DATA / 'SST/stsa.fine.train'\n",
        "dev_filepath = PATH_TO_DATA / 'SST/stsa.fine.dev'\n",
        "test_filepath = PATH_TO_DATA / 'SST/stsa.fine.test.X'\n",
        "def create_dataset(path,test=False):\n",
        "    loader = open(path,\"r\")\n",
        "    X=[]\n",
        "    y=[]\n",
        "    for sentence in loader:\n",
        "        sentence=re.sub(r'\\W+', ' ', sentence)\n",
        "        if test:\n",
        "            X+=[sentence.strip('\\n')]\n",
        "        else:\n",
        "            y+=[int(sentence[0])]\n",
        "            X+=[sentence[1:].strip('\\n')]\n",
        "    loader.close()\n",
        "    if test:\n",
        "        return X\n",
        "    else:\n",
        "        return X,y\n",
        "train_s,y_train=create_dataset(train_filepath)\n",
        "val_s,y_val=create_dataset(dev_filepath)\n",
        "test_s=create_dataset(test_filepath,test=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ct6hZaTDPgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2 - Encode sentences with the BoV model above\n",
        "#we'll Use idf weights as this seems to improve performance\n",
        "idf = sentence2vec.build_idf(np.concatenate((train_s,val_s,test_s)))\n",
        "def encode(sentences,BoV,idf=None):\n",
        "    embeddings=[]\n",
        "    for sentence in sentences:\n",
        "        embeddings+=[BoV.encode(sentence,idf)]\n",
        "    return np.vstack(embeddings)\n",
        "X_train=encode(train_s,sentence2vec,idf)\n",
        "X_val=encode(val_s,sentence2vec,idf)\n",
        "X_test=encode(test_s,sentence2vec,idf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "c-PIWSaKDPgN",
        "colab_type": "code",
        "outputId": "5ae05b34-822c-4240-8976-8ed8afefd34c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        }
      },
      "source": [
        "# 3 - Learn Logistic Regression on top of sentence embeddings using scikit-learn\n",
        "#     (consider tuning the L2 regularization on the dev set)\n",
        "#     In the paper, the accuracy for average of word vectors is 32.7%\n",
        "#     (VecAvg, table 1, https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n",
        "\n",
        "#we'll just use cross Validation here but we could've also tried using the score on the dev set as mentioned\n",
        "from sklearn.linear_model import LogisticRegression,LogisticRegressionCV\n",
        "from sklearn.metrics import classification_report\n",
        "Cs=np.linspace(3,4,20) # these values are chosen after first testing over larger increments \n",
        "clf = LogisticRegressionCV(Cs=Cs,multi_class='multinomial',solver='lbfgs',max_iter=1000,n_jobs=-1,cv=5)\n",
        "clf.fit(X_train, y_train)\n",
        "print(classification_report(y_val,clf.predict(X_val)))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      0.19      0.24       139\n",
            "           1       0.43      0.55      0.48       289\n",
            "           2       0.35      0.18      0.24       229\n",
            "           3       0.40      0.58      0.47       279\n",
            "           4       0.50      0.39      0.44       165\n",
            "\n",
            "    accuracy                           0.41      1101\n",
            "   macro avg       0.40      0.38      0.37      1101\n",
            "weighted avg       0.40      0.41      0.39      1101\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAyWKUT8DPgQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 4 - Produce 2210 predictions for the test set (in the same order). One line = one prediction (=0,1,2,3,4).\n",
        "#     Attach the output file \"logreg_bov_y_test_sst.txt\" to your deliverable.\n",
        "#     You will be evaluated on the results of the test set.\n",
        "pred=[str(i)+'\\n' for i in clf.predict(X_test)]\n",
        "file = open('logreg_bow_y_test_sst.txt',\"w\")\n",
        "file.writelines(pred)\n",
        "file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWs0kdBODPgS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "outputId": "d0a71b41-2b0a-4791-c935-e7226fd735d8"
      },
      "source": [
        "# BONUS!\n",
        "# 5 - Try to improve performance with another classifier\n",
        "#we'll use XGBoost\n",
        "#     Attach the output file \"XXX_bov_y_test_sst.txt\" to your deliverable (where XXX = the name of the classifier)\n",
        "# XGBoost \n",
        "import xgboost as xgb\n",
        "clf = xgb.XGBClassifier(n_jobs=12,max_depth = 3,n_estimators = 290) #Parameters chosen after doing cross validation \n",
        "clf.fit(X_train,y_train)\n",
        "print(classification_report(y_val,clf.predict(X_val)))\n",
        "#Outputting Prediction \n",
        "pred=[str(i)+'\\n' for i in clf.predict(X_test)]\n",
        "file = open('XXX_bow_y_test_sst.txt',\"w\")\n",
        "file.writelines(pred)\n",
        "file.close()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.48      0.23      0.31       139\n",
            "           1       0.41      0.55      0.47       289\n",
            "           2       0.45      0.19      0.27       229\n",
            "           3       0.40      0.61      0.48       279\n",
            "           4       0.48      0.36      0.41       165\n",
            "\n",
            "    accuracy                           0.42      1101\n",
            "   macro avg       0.44      0.39      0.39      1101\n",
            "weighted avg       0.43      0.42      0.40      1101\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "an-lg0rFDPgd",
        "colab_type": "text"
      },
      "source": [
        "# 4) Sentence classification with LSTMs in Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjhc5AGwDPge",
        "colab_type": "text"
      },
      "source": [
        "## 4.1 - Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhOXqTW0DPgf",
        "colab_type": "code",
        "outputId": "05f43b9a-1763-4d0c-9886-fef8a8ed5834",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# 1 - Using the same dataset, transform text to integers using tf.keras.preprocessing.text.one_hot function\n",
        "#     https://keras.io/preprocessing/text/\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from collections import Counter\n",
        "def Tokenize(sentences,words=None):\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        #The sentences will be stored as a list of words/tokens\n",
        "        sentences[i] = []\n",
        "        for word in nltk.word_tokenize(sentence): #Tokenizing the words\n",
        "            if words!=None:\n",
        "                words.update([word.lower()]) #Converting all the words to lower case\n",
        "            sentences[i].append(word)\n",
        "    return sentences\n",
        "\n",
        "def one_hot(sentences):\n",
        "    encoding=[]\n",
        "    for sentence in sentences:\n",
        "        encoding+=[torch.Tensor(tf.keras.preprocessing.text.one_hot(sentence,19538))]#19538 is the total number of words in dataset \n",
        "    return encoding \n",
        "\n",
        "\n",
        "words = Counter() #Dictionary that will map a word to the number of times it appeared in all the training sentences\n",
        "train_seq=Tokenize(train_s,words)\n",
        "val_seq=Tokenize(val_s)\n",
        "test_seq=Tokenize(test_s)\n",
        "\n",
        "# Removing the words that only appear once\n",
        "words = {k:v for k,v in words.items() if v>1}\n",
        "# Sorting the words according to the number of appearances, with the most common word being first\n",
        "words = sorted(words, key=words.get, reverse=True)\n",
        "# Adding padding and unknown to our vocabulary so that they will be assigned an index\n",
        "words = ['_PAD','_UNK'] + words\n",
        "# Dictionaries to store the word to index mappings and vice versa\n",
        "word2idx = {o:i for i,o in enumerate(words)}\n",
        "idx2word = {i:o for i,o in enumerate(words)}\n",
        "\n",
        "def one_hot(sentences,word2idx,unknown_index=1):\n",
        "    encoding=[]\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        # Looking up the mapping dictionary and assigning the index to the respective words\n",
        "        encoding+=[[word2idx[word] if word in word2idx else unknown_index for word in sentence]]\n",
        "    return encoding\n",
        "\n",
        "train_sentences=one_hot(train_seq,word2idx)\n",
        "val_sentences=one_hot(val_seq,word2idx)\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIgW8L8uDPgh",
        "colab_type": "text"
      },
      "source": [
        "**Padding input data**\n",
        "\n",
        "Models in Keras (and elsewhere) take batches of sentences of the same length as input. It is because Deep Learning framework have been designed to handle well Tensors, which are particularly suited for fast computation on the GPU.\n",
        "\n",
        "Since sentences have different sizes, we \"pad\" them. That is, we add dummy \"padding\" tokens so that they all have the same length.\n",
        "\n",
        "The input to a Keras model thus has this size : (batchsize, maxseqlen) where maxseqlen is the maximum length of a sentence in the batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EGzcJO4DPgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2 - Pad your sequences using tf.keras.preprocessing.sequence.pad_sequences\n",
        "#     https://keras.io/preprocessing/sequence/ \n",
        "# Defining a function that either shortens sentences or pads sentences with 0 to a fixed length \n",
        "def pad_input(sentences, seq_len,pad_idx=0): \n",
        "    features = np.zeros((len(sentences), seq_len),dtype=int)+pad_idx \n",
        "    for ii, sentence in enumerate(sentences):\n",
        "        if len(sentence) != 0:\n",
        "            features[ii, -len(sentence):] = np.array(sentence)[:seq_len]\n",
        "    return features\n",
        "seq_len = 52 #Max length on the set \n",
        "train_sentences = pad_input(train_sentences, seq_len)\n",
        "val_sentences= pad_input(val_sentences, seq_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4h_aqs63DPgk",
        "colab_type": "text"
      },
      "source": [
        "## 4.2 - Design and train your model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CatCUglqDPgl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 3 - Design your encoder + classifier using tensorflow.keras.layers\n",
        "#     In Keras, Torch and other deep learning framework, we create a \"container\" which is the Sequential() module.\n",
        "#     Then we add components to this container : the lookup-table, the LSTM, the classifier etc.\n",
        "#     All of these components are contained in the Sequential() and are trained together.\n",
        "#     Note that the embedding layer is initialized randomly and does not take advantage of pre-trained word embeddings.\n",
        "\n",
        "import torch.nn as nn\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
        "        #self.fc = nn.Sequential(nn.Dropout(drop_prob),nn.Linear(hidden_dim, output_size))\n",
        "        \n",
        "        self.fc = nn.Sequential(nn.Dropout(drop_prob),nn.Linear(hidden_dim,hidden_dim),nn.BatchNorm1d(hidden_dim),\n",
        "                                nn.Dropout(drop_prob),nn.Linear(hidden_dim,output_size))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        x = x.long()\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        lstm_out = lstm_out.contiguous().view( -1,self.hidden_dim)\n",
        "        #print(lstm_out.shape)\n",
        "        out = self.fc(lstm_out)\n",
        "        out=out.reshape(x.size(0),-1,self.output_size)\n",
        "        return out[:,-1]\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
        "        self.hidden=hidden\n",
        "        return \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "2xM0zLI2DPgq",
        "colab_type": "code",
        "outputId": "4d65d397-4b3b-48c9-d593-dee981d42a51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "# 4 - Define your loss/optimizer/metrics\n",
        "\n",
        "is_cuda = torch.cuda.is_available()\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "vocab_size = 50000\n",
        "output_size = 5\n",
        "embedding_dim = 64\n",
        "hidden_dim = 32\n",
        "n_layers = 2\n",
        "\n",
        "model = Classifier(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "model.to(device)\n",
        "print(model)\n",
        "lr=0.001\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=1e-3)  "
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classifier(\n",
            "  (embedding): Embedding(50000, 64)\n",
            "  (lstm): LSTM(64, 32, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (fc): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (3): Dropout(p=0.5, inplace=False)\n",
            "    (4): Linear(in_features=32, out_features=5, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "NyNWCIsaDPgu",
        "colab_type": "code",
        "outputId": "cb80171b-af2f-4191-c176-6653bde611e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 5 - Train your model and find the best hyperparameters for your dev set\n",
        "#     you will be evaluated on the quality of your predictions on the test set\n",
        "#     Keras expects y_train and y_dev to be one-hot encodings of the labels, i.e. with shape=(n_samples, 5)\n",
        "import copy\n",
        "def Train_model(model,criterion,optimizer,loader,device,scheduler=None,mode='train'):\n",
        "  \"\"\"Model Training/Validation\n",
        "  ------------------------------\n",
        "  Output:\n",
        "  -----------\n",
        "  mean_loss: Mean loss over Epoch\"\"\"\n",
        "\n",
        "  if mode=='train':\n",
        "    model.train()\n",
        "  elif mode=='val':\n",
        "    model.eval()\n",
        "  else:\n",
        "    print('mode should be Train or Val')\n",
        "    return\n",
        "  losses=[]\n",
        "  Preds=[]\n",
        "  Actual=[]\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for inputs, labels in loader:\n",
        "    inputs, labels = inputs.to(device), labels.long().to(device)\n",
        "    model.init_hidden(inputs.size(0))\n",
        "    optimizer.zero_grad()\n",
        "    with torch.set_grad_enabled(mode=='train'):\n",
        "      output = model(inputs)\n",
        "      loss = criterion(output, labels)\n",
        "      _,pred=torch.max(output, 1)\n",
        "      if mode=='train':\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    losses.append(loss.item())\n",
        "    #Preds=np.append(Preds,pred.cpu().long().numpy())\n",
        "    #Actual=np.append(Actual,labels.cpu().long().numpy())\n",
        "    correct+=float(torch.sum(pred==labels.data))\n",
        "    total+=float(inputs.shape[0])\n",
        "\n",
        "  if mode =='train' and scheduler!=None:\n",
        "    scheduler.step()\n",
        "  mean_loss=np.mean(losses)\n",
        "  print('{} Loss {:.4f}  Acc : {:.2%}  '.format(mode,mean_loss,correct/total))\n",
        "  return correct/total\n",
        "#------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "train_data = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(np.array(y_train)))\n",
        "val_data= TensorDataset(torch.from_numpy(val_sentences), torch.from_numpy(np.array(y_val)))\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "epochs=30\n",
        "acc_t=[]\n",
        "acc_v=[]\n",
        "best_acc=0\n",
        "for epoch in range(epochs):\n",
        "  print('epoch: ',epoch)\n",
        "  acc_t+=[Train_model(model,criterion,optimizer,train_loader,device,mode='train')]\n",
        "  acc_v+=[Train_model(model,criterion,optimizer,val_loader,device,mode='val')]\n",
        "  if acc_v[-1]>best_acc:\n",
        "        best_acc=acc_v[-1]\n",
        "        best_w=copy.deepcopy(model.state_dict())\n",
        "print('Best Accuracy: ',best_acc)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:  0\n",
            "train Loss 1.6257  Acc : 24.50%  \n",
            "val Loss 1.5726  Acc : 27.25%  \n",
            "epoch:  1\n",
            "train Loss 1.5842  Acc : 26.35%  \n",
            "val Loss 1.5670  Acc : 27.79%  \n",
            "epoch:  2\n",
            "train Loss 1.5727  Acc : 27.36%  \n",
            "val Loss 1.5672  Acc : 27.61%  \n",
            "epoch:  3\n",
            "train Loss 1.5732  Acc : 27.06%  \n",
            "val Loss 1.5725  Acc : 26.88%  \n",
            "epoch:  4\n",
            "train Loss 1.5691  Acc : 27.47%  \n",
            "val Loss 1.5724  Acc : 26.70%  \n",
            "epoch:  5\n",
            "train Loss 1.5610  Acc : 27.90%  \n",
            "val Loss 1.5586  Acc : 28.97%  \n",
            "epoch:  6\n",
            "train Loss 1.5529  Acc : 28.99%  \n",
            "val Loss 1.5462  Acc : 31.43%  \n",
            "epoch:  7\n",
            "train Loss 1.5394  Acc : 30.71%  \n",
            "val Loss 1.5321  Acc : 31.61%  \n",
            "epoch:  8\n",
            "train Loss 1.5189  Acc : 32.28%  \n",
            "val Loss 1.5207  Acc : 33.51%  \n",
            "epoch:  9\n",
            "train Loss 1.4958  Acc : 33.49%  \n",
            "val Loss 1.5081  Acc : 34.15%  \n",
            "epoch:  10\n",
            "train Loss 1.4698  Acc : 35.42%  \n",
            "val Loss 1.4847  Acc : 35.60%  \n",
            "epoch:  11\n",
            "train Loss 1.4220  Acc : 38.35%  \n",
            "val Loss 1.4523  Acc : 36.69%  \n",
            "epoch:  12\n",
            "train Loss 1.3676  Acc : 40.64%  \n",
            "val Loss 1.4320  Acc : 38.69%  \n",
            "epoch:  13\n",
            "train Loss 1.3309  Acc : 42.26%  \n",
            "val Loss 1.4169  Acc : 38.69%  \n",
            "epoch:  14\n",
            "train Loss 1.2780  Acc : 44.42%  \n",
            "val Loss 1.4348  Acc : 39.24%  \n",
            "epoch:  15\n",
            "train Loss 1.2266  Acc : 46.88%  \n",
            "val Loss 1.4426  Acc : 39.24%  \n",
            "epoch:  16\n",
            "train Loss 1.1942  Acc : 47.41%  \n",
            "val Loss 1.4868  Acc : 39.06%  \n",
            "epoch:  17\n",
            "train Loss 1.1468  Acc : 49.25%  \n",
            "val Loss 1.4793  Acc : 38.42%  \n",
            "epoch:  18\n",
            "train Loss 1.1024  Acc : 50.14%  \n",
            "val Loss 1.5231  Acc : 38.06%  \n",
            "epoch:  19\n",
            "train Loss 1.0715  Acc : 52.49%  \n",
            "val Loss 1.5584  Acc : 38.15%  \n",
            "epoch:  20\n",
            "train Loss 1.0326  Acc : 53.69%  \n",
            "val Loss 1.5779  Acc : 35.42%  \n",
            "epoch:  21\n",
            "train Loss 0.9986  Acc : 56.33%  \n",
            "val Loss 1.6577  Acc : 36.88%  \n",
            "epoch:  22\n",
            "train Loss 0.9503  Acc : 58.13%  \n",
            "val Loss 1.7357  Acc : 37.24%  \n",
            "epoch:  23\n",
            "train Loss 0.9297  Acc : 59.90%  \n",
            "val Loss 1.7750  Acc : 37.60%  \n",
            "epoch:  24\n",
            "train Loss 0.8689  Acc : 62.63%  \n",
            "val Loss 1.8477  Acc : 37.42%  \n",
            "epoch:  25\n",
            "train Loss 0.8310  Acc : 65.09%  \n",
            "val Loss 1.9125  Acc : 36.78%  \n",
            "epoch:  26\n",
            "train Loss 0.8074  Acc : 65.81%  \n",
            "val Loss 2.0889  Acc : 37.15%  \n",
            "epoch:  27\n",
            "train Loss 0.7567  Acc : 69.10%  \n",
            "val Loss 2.1186  Acc : 36.24%  \n",
            "epoch:  28\n",
            "train Loss 0.7187  Acc : 70.56%  \n",
            "val Loss 2.0867  Acc : 34.97%  \n",
            "epoch:  29\n",
            "train Loss 0.6834  Acc : 72.51%  \n",
            "val Loss 2.2169  Acc : 34.24%  \n",
            "Best Accuracy:  0.3923705722070845\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fZfYhowDPg0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 6 - Generate your predictions on the test set using model.predict(x_test)\n",
        "\n",
        "model.load_state_dict(best_w)\n",
        "test_sentences=one_hot(test_seq,word2idx)\n",
        "test_sentences= pad_input(test_sentences, seq_len)\n",
        "test_data= TensorDataset(torch.from_numpy(test_sentences))\n",
        "test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "Preds=np.array([],dtype=int)\n",
        "for inputs in test_loader:\n",
        "    inputs= inputs[0].to(device)\n",
        "    model.init_hidden(inputs.size(0))\n",
        "    with torch.no_grad():\n",
        "        output = model(inputs)\n",
        "        _,pred=torch.max(output, 1)\n",
        "    Preds=np.append(Preds,pred.cpu().numpy())\n",
        "pred=[str(i)+'\\n' for i in Preds]\n",
        "file = open('logreg_lstm_y_test_sst.txt',\"w\")\n",
        "file.writelines(pred)\n",
        "file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tARWBC5_DPg8",
        "colab_type": "text"
      },
      "source": [
        "## 4.3 - innovate !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_WvrBbShiKG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5abbc6ac-18ce-4e35-b2dc-3498a1928882"
      },
      "source": [
        "# 7 - Open question: find a model that is better on your dev set\n",
        "#     (e.g: use a 1D ConvNet, use a better classifier, pretrain your lookup tables ..)\n",
        "#     you will get point if the results on the test set are better: be careful of not overfitting your dev set too much..\n",
        "#     Attach the output file \"XXX_XXX_y_test_sst.txt\" to your deliverable.\n",
        "#-------------------------------------------------------------------------------------------------------------------\n",
        "#We'll Use Gensim Library \n",
        "from gensim.test.utils import datapath, get_tmpfile \n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "#-------------------------------------------------------------------------------------------------------------------\n",
        "#Downloading the publicly available embeddings  to data Directory\n",
        "!wget  http://nlp.stanford.edu/data/glove.840B.300d.zip -P data \n",
        "!unzip data/glove.840B.300d.zip -d data\n",
        "#-------------------------------------------------------------------------------------------------------------------\n",
        "#Loading Pretraine d EMbedding \n",
        "glove_file = r'data/glove.840B.300d.txt'\n",
        "#temporary file for word2vec\n",
        "open('data/word2vec.txt', 'a').close()\n",
        "tmp_file ='data/word2vec.txt'\n",
        "#tmp_file = get_tmpfile(r'data/word2vec.txt')\n",
        "word2vec = glove2word2vec(glove_file, tmp_file)\n",
        "glove = KeyedVectors.load_word2vec_format(tmp_file)\n",
        "embed_dim=glove.vectors.shape[1]\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------------------------\n",
        "#Recreating Dataset based on Indexes of Pretrained embedding \n",
        "#-------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "train_s,y_train=create_dataset(train_filepath)\n",
        "val_s,y_val=create_dataset(dev_filepath)\n",
        "words = Counter() #Dictionary that will map a word to the number of times it appeared in all the training sentences\n",
        "train_s=Tokenize(train_s,words)\n",
        "val_s=Tokenize(val_s)\n",
        "\n",
        "# Adding padding and unknown to our vocabulary so that they will be assigned an index\n",
        "extra=['_PAD','_UNK']\n",
        "words = extra + list(words)\n",
        "\n",
        "#Creating word2idx for Pretrained Glove EMbeddings\n",
        "glove.add(extra,[np.zeros(embed_dim),np.random.rand(embed_dim)*2-1])\n",
        "\n",
        "#Creating Tensor for Pytorch Embedding\n",
        "weights = torch.FloatTensor(glove.vectors).to(device)\n",
        "\n",
        "# Dictionaries to store the word to index mappings and vice versa\n",
        "word2idx={}\n",
        "for word in words:\n",
        "    if word in glove: \n",
        "        word2idx[word] =glove.vocab[word].index\n",
        "    else:\n",
        "        word2idx[word] =glove.vocab['_UNK'].index\n",
        "        \n",
        "train_sentences=one_hot(train_s,word2idx,unknown_index=glove.vocab['_UNK'].index)\n",
        "val_sentences=one_hot(val_s,word2idx,unknown_index=glove.vocab['_UNK'].index)\n",
        "train_sentences = pad_input(train_sentences, seq_len,pad_idx=glove.vocab['_PAD'].index)\n",
        "val_sentences= pad_input(val_sentences, seq_len,pad_idx=glove.vocab['_PAD'].index)\n",
        "\n",
        "\n",
        "\n",
        "#Wrappers for Pytorch Data Loaders\n",
        "train_data = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(np.array(y_train)))\n",
        "val_data= TensorDataset(torch.from_numpy(val_sentences), torch.from_numpy(np.array(y_val)))\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "#Model -------------------------------------------------------------------------------------------------------------------\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, output_size, hidden_dim, n_layers,weights, drop_prob=0.5,embed_dim=50):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding =nn.Embedding.from_pretrained(weights)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
        "        self.fc = nn.Sequential(nn.Linear(hidden_dim, output_size))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        x = x.long()\n",
        "        \n",
        "        embeds = self.embedding(x)\n",
        "        #print(embeds.shape)\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        lstm_out = lstm_out.contiguous().view( -1,self.hidden_dim)\n",
        "        #print(lstm_out.shape)\n",
        "        out = self.fc(lstm_out)\n",
        "        out=out.reshape(x.size(0),-1,self.output_size)\n",
        "        return out[:,-1]\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
        "        self.hidden=hidden\n",
        "        return \n",
        "#-------------------------------------------------------------------------------------------------------------------\n",
        "output_size = 5\n",
        "hidden_dim = 64\n",
        "n_layers = 2\n",
        "model = Classifier(output_size, hidden_dim, n_layers,weights,embed_dim=embed_dim)\n",
        "model.to(device)\n",
        "print(model)\n",
        "lr=0.0005\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=1e-3)  \n",
        "\n",
        "#Freezing embedding Weights : No grad\n",
        "model.embedding.weight.requires_grad=False\n",
        "#Training Model\n",
        "epochs=30\n",
        "acc_t=[]\n",
        "acc_v=[]\n",
        "best_acc=0\n",
        "for epoch in range(epochs):\n",
        "  print('epoch: ',epoch)\n",
        "  acc_t+=[Train_model(model,criterion,optimizer,train_loader,device,mode='train')]\n",
        "  acc_v+=[Train_model(model,criterion,optimizer,val_loader,device,mode='val')]\n",
        "  if acc_v[-1]>best_acc:\n",
        "        best_acc=acc_v[-1]\n",
        "        best_w=copy.deepcopy(model.state_dict())\n",
        "print('best accuracy on Validation set',best_acc)\n",
        "#-------------------------------------------------------------------------------------------------------------------\n",
        "#Create Prediction For test set\n",
        "model.load_state_dict(best_w)\n",
        "test_s=create_dataset(test_filepath,test=True)\n",
        "test_s=Tokenize(test_s)\n",
        "test_sentences=one_hot(test_s,word2idx,unknown_index=glove.vocab['_UNK'].index)\n",
        "test_sentences= pad_input(test_sentences, seq_len,pad_idx=glove.vocab['_PAD'].index)\n",
        "test_data= TensorDataset(torch.from_numpy(test_sentences))\n",
        "test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "Preds=np.array([],dtype=int)\n",
        "for inputs in test_loader:\n",
        "    inputs= inputs[0].to(device)\n",
        "    model.init_hidden(inputs.size(0))\n",
        "    with torch.no_grad():\n",
        "        output = model(inputs)\n",
        "        _,pred=torch.max(output, 1)\n",
        "    Preds=np.append(Preds,pred.cpu().numpy())\n",
        "pred=[str(i)+'\\n' for i in Preds]\n",
        "file = open('XXX_XXX_y_test_sst.txt',\"w\")\n",
        "file.writelines(pred)\n",
        "file.close()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classifier(\n",
            "  (embedding): Embedding(2196018, 300)\n",
            "  (lstm): LSTM(300, 64, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=64, out_features=5, bias=True)\n",
            "  )\n",
            ")\n",
            "epoch:  0\n",
            "train Loss 1.5210  Acc : 31.65%  \n",
            "val Loss 1.3878  Acc : 36.78%  \n",
            "epoch:  1\n",
            "train Loss 1.3563  Acc : 40.58%  \n",
            "val Loss 1.3383  Acc : 39.87%  \n",
            "epoch:  2\n",
            "train Loss 1.3128  Acc : 42.04%  \n",
            "val Loss 1.3206  Acc : 40.42%  \n",
            "epoch:  3\n",
            "train Loss 1.2752  Acc : 43.49%  \n",
            "val Loss 1.3164  Acc : 40.33%  \n",
            "epoch:  4\n",
            "train Loss 1.2528  Acc : 44.73%  \n",
            "val Loss 1.3453  Acc : 41.14%  \n",
            "epoch:  5\n",
            "train Loss 1.2358  Acc : 46.02%  \n",
            "val Loss 1.3001  Acc : 42.51%  \n",
            "epoch:  6\n",
            "train Loss 1.2148  Acc : 47.17%  \n",
            "val Loss 1.2767  Acc : 43.60%  \n",
            "epoch:  7\n",
            "train Loss 1.2028  Acc : 47.34%  \n",
            "val Loss 1.3198  Acc : 42.14%  \n",
            "epoch:  8\n",
            "train Loss 1.1893  Acc : 48.34%  \n",
            "val Loss 1.2892  Acc : 42.78%  \n",
            "epoch:  9\n",
            "train Loss 1.1742  Acc : 48.67%  \n",
            "val Loss 1.2887  Acc : 43.87%  \n",
            "epoch:  10\n",
            "train Loss 1.1686  Acc : 48.95%  \n",
            "val Loss 1.3205  Acc : 43.42%  \n",
            "epoch:  11\n",
            "train Loss 1.1533  Acc : 49.93%  \n",
            "val Loss 1.2819  Acc : 44.96%  \n",
            "epoch:  12\n",
            "train Loss 1.1356  Acc : 51.05%  \n",
            "val Loss 1.2986  Acc : 44.32%  \n",
            "epoch:  13\n",
            "train Loss 1.1253  Acc : 51.17%  \n",
            "val Loss 1.3026  Acc : 43.42%  \n",
            "epoch:  14\n",
            "train Loss 1.1058  Acc : 52.55%  \n",
            "val Loss 1.3113  Acc : 43.23%  \n",
            "epoch:  15\n",
            "train Loss 1.0894  Acc : 53.42%  \n",
            "val Loss 1.3378  Acc : 42.96%  \n",
            "epoch:  16\n",
            "train Loss 1.0759  Acc : 53.59%  \n",
            "val Loss 1.3123  Acc : 45.05%  \n",
            "epoch:  17\n",
            "train Loss 1.0581  Acc : 54.63%  \n",
            "val Loss 1.3625  Acc : 43.23%  \n",
            "epoch:  18\n",
            "train Loss 1.0469  Acc : 55.18%  \n",
            "val Loss 1.4381  Acc : 41.78%  \n",
            "epoch:  19\n",
            "train Loss 1.0278  Acc : 56.13%  \n",
            "val Loss 1.4113  Acc : 41.96%  \n",
            "epoch:  20\n",
            "train Loss 1.0089  Acc : 57.33%  \n",
            "val Loss 1.3888  Acc : 43.60%  \n",
            "epoch:  21\n",
            "train Loss 0.9947  Acc : 58.02%  \n",
            "val Loss 1.4383  Acc : 42.51%  \n",
            "epoch:  22\n",
            "train Loss 0.9834  Acc : 58.02%  \n",
            "val Loss 1.4806  Acc : 41.96%  \n",
            "epoch:  23\n",
            "train Loss 0.9530  Acc : 59.59%  \n",
            "val Loss 1.4890  Acc : 43.14%  \n",
            "epoch:  24\n",
            "train Loss 0.9385  Acc : 60.59%  \n",
            "val Loss 1.4355  Acc : 43.51%  \n",
            "epoch:  25\n",
            "train Loss 0.9140  Acc : 61.33%  \n",
            "val Loss 1.5053  Acc : 41.78%  \n",
            "epoch:  26\n",
            "train Loss 0.8975  Acc : 63.17%  \n",
            "val Loss 1.5419  Acc : 42.42%  \n",
            "epoch:  27\n",
            "train Loss 0.8774  Acc : 63.75%  \n",
            "val Loss 1.6130  Acc : 42.14%  \n",
            "epoch:  28\n",
            "train Loss 0.8566  Acc : 64.52%  \n",
            "val Loss 1.5623  Acc : 41.78%  \n",
            "epoch:  29\n",
            "train Loss 0.8324  Acc : 65.67%  \n",
            "val Loss 1.5370  Acc : 42.05%  \n",
            "best accuracy on Validation set 0.45049954586739327\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}