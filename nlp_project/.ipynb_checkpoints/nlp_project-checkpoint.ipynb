{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP - Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RULES:\n",
    "\n",
    "* Do not create any additional cell\n",
    "\n",
    "* Fill in the blanks\n",
    "\n",
    "* All cells should be runnable (modulo trivial compatibility bugs that we'd fix)\n",
    "\n",
    "* 4 / 20 points will be allocated to the clarity of your code\n",
    "\n",
    "* Efficient code will have a bonus\n",
    "\n",
    "DELIVERABLE:\n",
    "\n",
    "* the pdf with your answers\n",
    "* this notebook\n",
    "* the predictions of the SST test set\n",
    "\n",
    "DO NOT INCLUDE THE DATASETS IN THE DELIVERABLE.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Python 3.6 or above is required\n",
    "from collections import defaultdict\n",
    "import gzip\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve\n",
    "import re \n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = Path('data/')\n",
    "# Download word vectors, might take a few minutes and about ~3GB of storage space\n",
    "en_embeddings_path = PATH_TO_DATA / 'cc.en.300.vec.gz'\n",
    "if not en_embeddings_path.exists():\n",
    "    urlretrieve('https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz', en_embeddings_path)\n",
    "fr_embeddings_path = PATH_TO_DATA / 'cc.fr.300.vec.gz'\n",
    "if not fr_embeddings_path.exists():\n",
    "    urlretrieve('https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.vec.gz', fr_embeddings_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Monolingual (English) word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec():\n",
    "\n",
    "    def __init__(self, filepath, vocab_size=50000):\n",
    "        self.words, self.embeddings = self.load_wordvec(filepath, vocab_size)\n",
    "        # Mappings for O(1) retrieval:\n",
    "        self.word2id = {word: idx for idx, word in enumerate(self.words)}\n",
    "        self.id2word = {idx: word for idx, word in enumerate(self.words)}\n",
    "    \n",
    "    def load_wordvec(self, filepath, vocab_size):\n",
    "        assert str(filepath).endswith('.gz')\n",
    "        words = []\n",
    "        embeddings = []\n",
    "        with gzip.open(filepath, 'rt',encoding=\"utf8\") as f:  # Read compressed file directly\n",
    "            next(f)  # Skip header\n",
    "            for i, line in enumerate(f):\n",
    "                word, vec = line.split(' ', 1)\n",
    "                words.append(word)\n",
    "                embeddings.append(np.fromstring(vec, sep=' '))\n",
    "                if i == (vocab_size - 1):\n",
    "                    break\n",
    "        print('Loaded %s pretrained word vectors' % (len(words)))\n",
    "        return words, np.vstack(embeddings)\n",
    "    \n",
    "    def encode(self, word):\n",
    "        \n",
    "        # Returns the 1D embedding of a given word\n",
    "        try:\n",
    "            return self.embeddings[self.word2id[word]]\n",
    "        except KeyError:\n",
    "            #print(\"word \",word, \" not in vocab\")\n",
    "            return np.zeros(self.embeddings[0].shape)\n",
    "        \n",
    "    \n",
    "    def score(self, word1, word2):\n",
    "        # Return the cosine similarity: use np.dot & np.linalg.norm\n",
    "        v1=self.encode(word1)\n",
    "        v2=self.encode(word2)\n",
    "        return (v1.T@v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))\n",
    "    \n",
    "    def most_similar(self, word, k=5):\n",
    "        # Returns the k most similar words: self.score & np.argsort\n",
    "        scores=[self.score(word,vocab) for vocab in self.words]\n",
    "        sorted_scores=np.flip(np.argsort(scores,axis=None))\n",
    "        return [self.id2word[idx] for idx in sorted_scores[1:k+1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 pretrained word vectors\n",
      "cat tree 0.26449754661654756\n",
      "cat dog 0.7078641298542564\n",
      "cat pet 0.6753313359976382\n",
      "Paris France 0.6892958925806543\n",
      "Paris Germany 0.4051242286737549\n",
      "Paris baguette 0.29399958277802224\n",
      "Paris donut -0.006588507552348003\n",
      "['cats', 'kitty', 'kitten', 'feline', 'dog']\n",
      "['dogs', 'puppy', 'pup', 'canine', 'pet']\n",
      "['dog', 'cats', 'puppies', 'Dogs', 'pets']\n",
      "['France', 'Parisian', 'Marseille', 'Brussels', 'Strasbourg']\n",
      "['Austria', 'Europe', 'Berlin', 'Hamburg', 'Bavaria']\n"
     ]
    }
   ],
   "source": [
    "word2vec = Word2Vec(en_embeddings_path, vocab_size=50000)\n",
    "# You will be evaluated on the output of the following:\n",
    "for word1, word2 in zip(('cat', 'cat', 'cat', 'Paris', 'Paris', 'Paris', 'Paris'), ('tree', 'dog', 'pet', 'France', 'Germany', 'baguette', 'donut')):\n",
    "    print(word1, word2, word2vec.score(word1, word2))\n",
    "for word in ['cat', 'dog', 'dogs', 'Paris', 'Germany']:\n",
    "    print(word2vec.most_similar(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class BagOfWords():\n",
    "    \n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "    \n",
    "    def build_idf(self, sentences):\n",
    "        # build the idf dictionary: associate each word to its idf value\n",
    "        # -> idf = {word: idf_value, ...}\n",
    "        occ=defaultdict(float)\n",
    "        for sentence in sentences:\n",
    "            words=np.unique([word for word in re.sub(r'\\W+', ' ', sentence).split()])\n",
    "            for word in words:\n",
    "                occ[word]+=1\n",
    "        D=len(sentences)\n",
    "        idf={k:np.log(D/v) for k,v in occ.items()}\n",
    "        return idf\n",
    "    \n",
    "    def encode(self, sentence, idf=None):\n",
    "        # Takes a sentence as input, returns the sentence embedding\n",
    "        # mean of word vectors\n",
    "        words=[word for word in re.sub(r'\\W+', ' ', sentence).split()]\n",
    "        if idf is None:\n",
    "            words=[self.word2vec.encode(word) for word in words]\n",
    "            return np.average(words,axis=0)\n",
    "        else:\n",
    "            # idf-weighted mean of word vectors\n",
    "            words=[self.word2vec.encode(word)*idf[word] for word in words]\n",
    "            return np.average(words,axis=0)\n",
    "\n",
    "    def score(self, sentence1, sentence2, idf=None):\n",
    "        # Return the cosine similarity: use np.dot & np.linalg.norm\n",
    "        v1=self.encode(sentence1,idf)\n",
    "        v2=self.encode(sentence2,idf)\n",
    "        return (v1.T@v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))\n",
    "    def most_similar(self, sentence, sentences, idf=None, k=5):\n",
    "        # Return most similar sentences\n",
    "        query = self.encode(sentence, idf)\n",
    "        keys = np.vstack([self.encode(sentence, idf) for sentence in sentences])\n",
    "        scores=[self.score(sentence,vocab,idf) for vocab in sentences]\n",
    "        sorted_scores=np.flip(np.argsort(scores,axis=None))\n",
    "        return [sentences[idx] for idx in sorted_scores[1:k+1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000000 pretrained word vectors\n",
      "\n",
      "\tAverage of word embeddings\n",
      "1 man singing and 1 man playing a saxophone in a concert . \n",
      "10 people venture out to go crosscountry skiing . \n",
      "0.6405950330419071\n",
      "1 smiling african american boy . \n",
      "1) 2 woman dancing while pointing . \n",
      "2) 5 women and 1 man are smiling for the camera . \n",
      "3) 3 males and 1 woman enjoying a sporting event \n",
      "4) 2 chinese people wearing traditional clothes \n",
      "5) a young boy and 2 girls open christmas presents . \n",
      "\n",
      "\tidf weighted average of word embeddings\n",
      "1 man singing and 1 man playing a saxophone in a concert . \n",
      "10 people venture out to go crosscountry skiing . \n",
      "0.636915654001658\n",
      "1 smiling african american boy . \n",
      "1) 1 man singing and 1 man playing a saxophone in a concert . \n",
      "2) two women and 1 man walking across the street . \n",
      "3) 2 guys facing away from camera , 1 girl smiling at camera with blue shirt , 1 guy with a beverage with a jacket on . \n",
      "4) 3 males and 1 woman enjoying a sporting event \n",
      "5) 5 women and 1 man are smiling for the camera . \n"
     ]
    }
   ],
   "source": [
    "word2vec = Word2Vec(en_embeddings_path, vocab_size=2000000)\n",
    "sentence2vec = BagOfWords(word2vec)\n",
    "\n",
    "# Load sentences in \"PATH_TO_DATA/sentences.txt\"\n",
    "filepath = PATH_TO_DATA / 'sentences.txt'\n",
    "with open(filepath, 'r') as f:\n",
    "    sentences = [line.strip('\\n') for line in f]\n",
    "\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "print('\\n\\tAverage of word embeddings')\n",
    "sentence1 = sentences[7]\n",
    "sentence2 = sentences[13]\n",
    "print(sentence1)\n",
    "print(sentence2)\n",
    "print(sentence2vec.score(sentence1, sentence2))\n",
    "sentence = sentences[10]\n",
    "similar_sentences = sentence2vec.most_similar(sentence, sentences)  # BagOfWords-mean\n",
    "print(sentence)\n",
    "\n",
    "for i, sentence in enumerate(similar_sentences):\n",
    "    print(str(i+1) + ')', sentence)\n",
    "    \n",
    "# Build idf scores for each word\n",
    "idf = sentence2vec.build_idf(sentences)\n",
    "\n",
    "\n",
    "print('\\n\\tidf weighted average of word embeddings')\n",
    "print(sentence1)\n",
    "print(sentence2)\n",
    "print(sentence2vec.score(sentence1, sentence2, idf))\n",
    "sentence = sentences[10]\n",
    "similar_sentences = sentence2vec.most_similar(sentence, sentences, idf)  # BagOfWords-idf\n",
    "print(sentence)\n",
    "for i, sentence in enumerate(similar_sentences):\n",
    "    print(str(i+1) + ')', sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Multilingual (English-French) word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a bilingual dictionary of size V_a (e.g French-English).\n",
    "\n",
    "Let's define **X** and **Y** the **French** and **English** matrices.\n",
    "\n",
    "They contain the embeddings associated to the words in the bilingual dictionary.\n",
    "\n",
    "We want to find a **mapping W** that will project the source word space (e.g French) to the target word space (e.g English).\n",
    "\n",
    "Procrustes : **W\\* = argmin || W.X - Y ||  s.t  W^T.W = Id**\n",
    "has a closed form solution:\n",
    "**W = U.V^T  where  U.Sig.V^T = SVD(Y.X^T)**\n",
    "\n",
    "In what follows, you are asked to: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilingualWordAligner:\n",
    "    \n",
    "    def __init__(self, fr_word2vec, en_word2vec):\n",
    "        self.fr_word2vec = fr_word2vec\n",
    "        self.en_word2vec = en_word2vec\n",
    "        self.aligned_fr_embeddings = self.get_aligned_fr_embeddings()\n",
    "        \n",
    "    def get_aligned_fr_embeddings(self):\n",
    "        # 1 - Get words that appear in both vocabs (= identical character strings)\n",
    "        identical=[]\n",
    "        for word in self.fr_word2vec.words:\n",
    "            if word in self.en_word2vec.word2id: #this is in O(1) in a dict \n",
    "                identical+=[word]\n",
    "        X,Y=[],[]\n",
    "        for word in identical:\n",
    "            X+=[self.fr_word2vec.encode(word)]\n",
    "            Y+=[self.en_word2vec.encode(word)]\n",
    "        X=np.vstack(X).T\n",
    "        Y=np.vstack(Y).T\n",
    "        #     Use it to create the matrix X (emb_dim, vocab_size) and Y (emb_dim, vocab_size) (of embeddings for these words)\n",
    "        assert X.shape[0] == 300 and Y.shape[0] == 300\n",
    "        \n",
    "        # 2 - Solve the Procrustes using the numpy package and: np.linalg.svd() and get the optimal W\n",
    "        U,_,Vh=np.linalg.svd(Y@X.T)\n",
    "        W=U@Vh\n",
    "        #     Now self.fr_word2vec.embeddings * W.transpose() is in the same space as en_word2vec.embeddings\n",
    "        assert W.shape == (300, 300)\n",
    "        return np.matmul(fr_word2vec.embeddings, W.transpose())\n",
    "        \n",
    "    def get_closest_english_words(self, fr_word, k=3):\n",
    "        # 3 - Return the top k English nearest neighbors to the input French word\n",
    "        en_embed=self.aligned_fr_embeddings[self.fr_word2vec.word2id[fr_word]]\n",
    "        #we'll add the French Word to The English Vocab along with its \"english embedding\"\n",
    "        vocab_size=len(self.en_word2vec.id2word)\n",
    "        self.en_word2vec.embeddings=np.vstack((self.en_word2vec.embeddings,en_embed))\n",
    "        self.en_word2vec.word2id[fr_word]=vocab_size\n",
    "        self.en_word2vec.id2word[vocab_size]=fr_word # although we won't need this for the closest words\n",
    "        return self.en_word2vec.most_similar(fr_word,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 pretrained word vectors\n",
      "Loaded 50000 pretrained word vectors\n",
      "----------\n",
      "fr: \"chat\"\n",
      "en: \"cat\"\n",
      "en: \"kitten\"\n",
      "en: \"kitty\"\n",
      "----------\n",
      "fr: \"chien\"\n",
      "en: \"dog\"\n",
      "en: \"cat\"\n",
      "en: \"pet\"\n",
      "----------\n",
      "fr: \"voiture\"\n",
      "en: \"vehicle\"\n",
      "en: \"automobile\"\n",
      "en: \"motorbike\"\n",
      "----------\n",
      "fr: \"zut\"\n",
      "en: \"Ah\"\n",
      "en: \"ah\"\n",
      "en: \"nope\"\n"
     ]
    }
   ],
   "source": [
    "fr_word2vec = Word2Vec(fr_embeddings_path, vocab_size=50000)\n",
    "en_word2vec = Word2Vec(en_embeddings_path, vocab_size=50000)\n",
    "multilingual_word_aligner = MultilingualWordAligner(fr_word2vec, en_word2vec)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "fr_words = ['chat', 'chien', 'voiture', 'zut']\n",
    "k = 3\n",
    "for fr_word in fr_words:\n",
    "    print('-' * 10)\n",
    "    print(f'fr: \"{fr_word}\"')\n",
    "    en_words = multilingual_word_aligner.get_closest_english_words(fr_word, k=3)\n",
    "    for en_word in en_words:\n",
    "        print(f'en: \"{en_word}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to dive deeper on this subject: https://github.com/facebookresearch/MUSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Sentence classification with BoV and scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test of Stanford Sentiment TreeBank (SST)\n",
    "#     (https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n",
    "train_filepath = PATH_TO_DATA / 'SST/stsa.fine.train'\n",
    "dev_filepath = PATH_TO_DATA / 'SST/stsa.fine.dev'\n",
    "test_filepath = PATH_TO_DATA / 'SST/stsa.fine.test.X'\n",
    "def create_dataset(path,test=False):\n",
    "    loader = open(path,\"r\")\n",
    "    X=[]\n",
    "    y=[]\n",
    "    for sentence in loader:\n",
    "        sentence=re.sub(r'\\W+', ' ', sentence)\n",
    "        if test:\n",
    "            X+=[sentence.strip('\\n')]\n",
    "        else:\n",
    "            y+=[int(sentence[0])]\n",
    "            X+=[sentence[1:].strip('\\n')]\n",
    "    loader.close()\n",
    "    if test:\n",
    "        return X\n",
    "    else:\n",
    "        return X,y\n",
    "train_s,y_train=create_dataset(train_filepath)\n",
    "val_s,y_val=create_dataset(dev_filepath)\n",
    "test_s=create_dataset(test_filepath,test=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Encode sentences with the BoV model above\n",
    "#we'll Use idf weights as this seems to improve performance\n",
    "idf = sentence2vec.build_idf(np.concatenate((train_s,val_s,test_s)))\n",
    "def encode(sentences,BoV,idf=None):\n",
    "    embeddings=[]\n",
    "    for sentence in sentences:\n",
    "        embeddings+=[BoV.encode(sentence,idf)]\n",
    "    return np.vstack(embeddings)\n",
    "X_train=encode(train_s,sentence2vec,idf)\n",
    "X_val=encode(val_s,sentence2vec,idf)\n",
    "X_test=encode(test_s,sentence2vec,idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.19      0.24       139\n",
      "           1       0.43      0.54      0.48       289\n",
      "           2       0.33      0.18      0.23       229\n",
      "           3       0.40      0.58      0.47       279\n",
      "           4       0.49      0.39      0.43       165\n",
      "\n",
      "    accuracy                           0.41      1101\n",
      "   macro avg       0.40      0.37      0.37      1101\n",
      "weighted avg       0.40      0.41      0.39      1101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 - Learn Logistic Regression on top of sentence embeddings using scikit-learn\n",
    "#     (consider tuning the L2 regularization on the dev set)\n",
    "#     In the paper, the accuracy for average of word vectors is 32.7%\n",
    "#     (VecAvg, table 1, https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n",
    "\n",
    "#we'll just use cross Validation here but we could've also tried using the score on the dev set as mentioned\n",
    "from sklearn.linear_model import LogisticRegression,LogisticRegressionCV\n",
    "from sklearn.metrics import classification_report\n",
    "Cs=np.linspace(3,4,20) # these values are chosen after first testing over larger increments \n",
    "clf = LogisticRegressionCV(Cs=Cs,multi_class='multinomial',solver='lbfgs',max_iter=1000,n_jobs=-1,cv=5)\n",
    "clf.fit(X_train, y_train)\n",
    "print(classification_report(y_val,clf.predict(X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 - Produce 2210 predictions for the test set (in the same order). One line = one prediction (=0,1,2,3,4).\n",
    "#     Attach the output file \"logreg_bov_y_test_sst.txt\" to your deliverable.\n",
    "#     You will be evaluated on the results of the test set.\n",
    "pred=[str(i)+'\\n' for i in clf.predict(X_test)]\n",
    "file = open('logreg_bov_y_test_sst.txt',\"w\")\n",
    "file.writelines(pred)\n",
    "file.close()\n",
    "# TYPE CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b02d96dcbdb9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#     Attach the output file \"XXX_bov_y_test_sst.txt\" to your deliverable (where XXX = the name of the classifier)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# XGBoost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_depth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m290\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Parameters chosen after doing cross validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Finance\\lib\\site-packages\\xgboost\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDMatrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBooster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrabit\u001b[0m                   \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Finance\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m from .compat import (STRING_TYPES, PY3, DataFrame, MultiIndex, py_str,\n\u001b[0m\u001b[0;32m     23\u001b[0m                      PANDAS_INSTALLED, DataTable)\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlibpath\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfind_lib_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Finance\\lib\\site-packages\\xgboost\\compat.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;31m# sklearn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBaseEstimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRegressorMixin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mClassifierMixin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Finance\\lib\\site-packages\\sklearn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Finance\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_IS_32BIT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m _DEFAULT_TAGS = {\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Finance\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmurmurhash\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmurmurhash3_32\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_joblib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataConversionWarning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdeprecation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Finance\\lib\\site-packages\\sklearn\\utils\\_joblib.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# joblib imports may raise DeprecationWarning on certain Python\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# versions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mjoblib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mjoblib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Finance\\lib\\site-packages\\joblib\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mnumpy_pickle\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcompressor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mregister_compressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mparallel\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mParallel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mparallel\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdelayed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mparallel\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcpu_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Finance\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmy_exceptions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTransportableException\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdisk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmemstr_to_bytes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m from ._parallel_backends import (FallbackToBackend, MultiprocessingBackend,\n\u001b[0m\u001b[0;32m     29\u001b[0m                                  \u001b[0mThreadingBackend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSequentialBackend\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                                  LokyBackend)\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Finance\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMemmappingPool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mThreadPool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_memmapping_executor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# Compat between concurrent.futures and multiprocessing TimeoutError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Finance\\lib\\site-packages\\joblib\\executor.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdisk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdelete_folder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_memmapping_reducer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_memmapping_reducers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mexternals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloky\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreusable_executor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_reusable_executor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Finance\\lib\\site-packages\\joblib\\externals\\loky\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcpu_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mset_loky_pickler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mreusable_executor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_reusable_executor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcloudpickle_wrapper\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwrap_non_picklable_objects\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mprocess_executor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBrokenProcessPool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mProcessPoolExecutor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Finance\\lib\\site-packages\\joblib\\externals\\loky\\reusable_executor.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmultiprocessing\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mprocess_executor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mProcessPoolExecutor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEXTRA_QUEUED_CALLS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcpu_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Finance\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueues\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mQueue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSimpleQueue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFull\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mset_loky_pickler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_loky_pickler_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrecursive_terminate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_exitcodes_terminated_worker\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Finance\\lib\\site-packages\\joblib\\externals\\loky\\backend\\utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mimport\u001b[0m \u001b[0mpsutil\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mpsutil\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Finance\\lib\\site-packages\\psutil\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0mWINDOWS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_pswindows\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_psplatform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_psutil_windows\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mABOVE_NORMAL_PRIORITY_CLASS\u001b[0m  \u001b[1;31m# NOQA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_psutil_windows\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBELOW_NORMAL_PRIORITY_CLASS\u001b[0m  \u001b[1;31m# NOQA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Finance\\lib\\site-packages\\psutil\\_pswindows.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_psutil_windows\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dll load failed\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# BONUS!\n",
    "# 5 - Try to improve performance with another classifier\n",
    "#we'll use XGBoost\n",
    "#     Attach the output file \"XXX_bov_y_test_sst.txt\" to your deliverable (where XXX = the name of the classifier)\n",
    "# XGBoost \n",
    "import xgboost as xgb\n",
    "clf = xgb.XGBClassifier(n_jobs=12,max_depth = 3,n_estimators = 290) #Parameters chosen after doing cross validation \n",
    "clf.fit(X_train,y_train)\n",
    "#Outputting Prediction \n",
    "pred=[str(i)+'\\n' for i in clf.predict(X_test)]\n",
    "file = open('XGBoost_bov_y_test_sst.txt',\"w\")\n",
    "file.writelines(pred)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Sentence classification with LSTMs in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Using the same dataset, transform text to integers using tf.keras.preprocessing.text.one_hot function\n",
    "#     https://keras.io/preprocessing/text/\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import nltk\n",
    "from collections import Counter\n",
    "def Tokenize(sentences,words=None):\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        #The sentences will be stored as a list of words/tokens\n",
    "        sentences[i] = []\n",
    "        for word in nltk.word_tokenize(sentence): #Tokenizing the words\n",
    "            if words!=None:\n",
    "                words.update([word.lower()]) #Converting all the words to lower case\n",
    "            sentences[i].append(word)\n",
    "    return sentences\n",
    "\n",
    "def one_hot(sentences):\n",
    "    encoding=[]\n",
    "    for sentence in sentences:\n",
    "        encoding+=[torch.Tensor(tf.keras.preprocessing.text.one_hot(sentence,19538))]#19538 is the total number of words in dataset \n",
    "    return encoding \n",
    "\n",
    "\n",
    "words = Counter() #Dictionary that will map a word to the number of times it appeared in all the training sentences\n",
    "train_s=Tokenize(train_s,words)\n",
    "val_s=Tokenize(val_s)\n",
    "test_s=Tokenize(test_s)\n",
    "\n",
    "# Removing the words that only appear once\n",
    "words = {k:v for k,v in words.items() if v>1}\n",
    "# Sorting the words according to the number of appearances, with the most common word being first\n",
    "words = sorted(words, key=words.get, reverse=True)\n",
    "# Adding padding and unknown to our vocabulary so that they will be assigned an index\n",
    "words = ['_PAD','_UNK'] + words\n",
    "# Dictionaries to store the word to index mappings and vice versa\n",
    "word2idx = {o:i for i,o in enumerate(words)}\n",
    "idx2word = {i:o for i,o in enumerate(words)}\n",
    "\n",
    "def one_hot(sentences,word2idx,unknown_index=1):\n",
    "    encoding=[]\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        # Looking up the mapping dictionary and assigning the index to the respective words\n",
    "        encoding+=[[word2idx[word] if word in word2idx else unknown_index for word in sentence]]\n",
    "    return encoding\n",
    "\n",
    "train_sentences=one_hot(train_s,word2idx)\n",
    "val_sentences=one_hot(val_s,word2idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding input data**\n",
    "\n",
    "Models in Keras (and elsewhere) take batches of sentences of the same length as input. It is because Deep Learning framework have been designed to handle well Tensors, which are particularly suited for fast computation on the GPU.\n",
    "\n",
    "Since sentences have different sizes, we \"pad\" them. That is, we add dummy \"padding\" tokens so that they all have the same length.\n",
    "\n",
    "The input to a Keras model thus has this size : (batchsize, maxseqlen) where maxseqlen is the maximum length of a sentence in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Pad your sequences using tf.keras.preprocessing.sequence.pad_sequences\n",
    "#     https://keras.io/preprocessing/sequence/\n",
    "# Defining a function that either shortens sentences or pads sentences with 0 to a fixed length\n",
    "def pad_input(sentences, seq_len,pad_idx=0):\n",
    "    features = np.zeros((len(sentences), seq_len),dtype=int)+pad_idx\n",
    "    for ii, sentence in enumerate(sentences):\n",
    "        if len(sentence) != 0:\n",
    "            features[ii, -len(sentence):] = np.array(sentence)[:seq_len]\n",
    "    return features\n",
    "seq_len = 52 #Max length on the set \n",
    "train_sentences = pad_input(train_sentences, seq_len)\n",
    "val_sentences= pad_input(val_sentences, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Design and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - Design your encoder + classifier using tensorflow.keras.layers\n",
    "#     In Keras, Torch and other deep learning framework, we create a \"container\" which is the Sequential() module.\n",
    "#     Then we add components to this container : the lookup-table, the LSTM, the classifier etc.\n",
    "#     All of these components are contained in the Sequential() and are trained together.\n",
    "#     Note that the embedding layer is initialized randomly and does not take advantage of pre-trained word embeddings.\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        #self.fc = nn.Sequential(nn.Dropout(drop_prob),nn.Linear(hidden_dim, output_size))\n",
    "        \n",
    "        self.fc = nn.Sequential(nn.Dropout(drop_prob),nn.Linear(hidden_dim,hidden_dim),nn.BatchNorm1d(hidden_dim),\n",
    "                                nn.Dropout(drop_prob),nn.Linear(hidden_dim,output_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.long()\n",
    "        \n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        lstm_out = lstm_out.contiguous().view( -1,self.hidden_dim)\n",
    "        #print(lstm_out.shape)\n",
    "        out = self.fc(lstm_out)\n",
    "        out=out.reshape(x.size(0),-1,self.output_size)\n",
    "        return out[:,-1]\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        self.hidden=hidden\n",
    "        return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-158-1aaa8db33707>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mn_layers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-152-e143e53fa1bb>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, output_size, hidden_dim, n_layers, weights, drop_prob)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_layers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop_prob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Finance\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, embeddings, freeze, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m \u001b[1;36m4.0000\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;36m5.1000\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;36m6.3000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         \"\"\"\n\u001b[1;32m--> 157\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m             \u001b[1;34m'Embeddings parameter is expected to be 2-dimensional'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[0mrows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "# 4 - Define your loss/optimizer/metrics\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "vocab_size = 50000\n",
    "output_size = 5\n",
    "embedding_dim = 64\n",
    "hidden_dim = 32\n",
    "n_layers = 2\n",
    "\n",
    "model = Classifier(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "model.to(device)\n",
    "print(model)\n",
    "lr=0.0005\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=1e-3)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-159-591f13e18eee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m   \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'epoch: '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m   \u001b[0macc_t\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[0macc_v\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0macc_v\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m>\u001b[0m\u001b[0mbest_acc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-159-591f13e18eee>\u001b[0m in \u001b[0;36mTrain_model\u001b[1;34m(model, criterion, optimizer, loader, device, scheduler, mode)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[1;31m#Preds=np.append(Preds,pred.cpu().long().numpy())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;31m#Actual=np.append(Actual,labels.cpu().long().numpy())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 5 - Train your model and find the best hyperparameters for your dev set\n",
    "#     you will be evaluated on the quality of your predictions on the test set\n",
    "#     Keras expects y_train and y_dev to be one-hot encodings of the labels, i.e. with shape=(n_samples, 5)\n",
    "import copy\n",
    "def Train_model(model,criterion,optimizer,loader,device,scheduler=None,mode='train'):\n",
    "  \"\"\"Model Training/Validation\n",
    "  ------------------------------\n",
    "  Output:\n",
    "  -----------\n",
    "  mean_loss: Mean loss over Epoch\"\"\"\n",
    "\n",
    "  if mode=='train':\n",
    "    model.train()\n",
    "  elif mode=='val':\n",
    "    model.eval()\n",
    "  else:\n",
    "    print('mode should be Train or Val')\n",
    "    return\n",
    "  losses=[]\n",
    "  Preds=[]\n",
    "  Actual=[]\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  for inputs, labels in loader:\n",
    "    inputs, labels = inputs.to(device), labels.long().to(device)\n",
    "    model.init_hidden(inputs.size(0))\n",
    "    optimizer.zero_grad()\n",
    "    with torch.set_grad_enabled(mode=='train'):\n",
    "      output = model(inputs)\n",
    "      loss = criterion(output, labels)\n",
    "      _,pred=torch.max(output, 1)\n",
    "      if mode=='train':\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "    #Preds=np.append(Preds,pred.cpu().long().numpy())\n",
    "    #Actual=np.append(Actual,labels.cpu().long().numpy())\n",
    "    correct+=float(torch.sum(pred==labels.data))\n",
    "    total+=float(inputs.shape[0])\n",
    "\n",
    "  if mode =='train' and scheduler!=None:\n",
    "    scheduler.step()\n",
    "  mean_loss=np.mean(losses)\n",
    "  print('{} Loss {:.4f}  Acc : {:.2%}  '.format(mode,mean_loss,correct/total))\n",
    "  return correct/total\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(np.array(y_train)))\n",
    "val_data= TensorDataset(torch.from_numpy(val_sentences), torch.from_numpy(np.array(y_val)))\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "epochs=30\n",
    "acc_t=[]\n",
    "acc_v=[]\n",
    "best_acc=0\n",
    "for epoch in range(epochs):\n",
    "  print('epoch: ',epoch)\n",
    "  acc_t+=[Train_model(model,criterion,optimizer,train_loader,device,mode='train')]\n",
    "  acc_v+=[Train_model(model,criterion,optimizer,val_loader,device,mode='val')]\n",
    "  if acc_v[-1]>best_acc:\n",
    "        best_acc=acc_v[-1]\n",
    "        best_w=copy.deepcopy(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1c61d801c48>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.plot(acc_t,label='Train Accuracy')\n",
    "plt.plot(acc_v,label='val Accuracy')\n",
    "plt.title('Accuracy Evolution')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 - Generate your predictions on the test set using model.predict(x_test)\n",
    "\n",
    "model.load_state_dict(best_w)\n",
    "test_sentences=one_hot(test_s,word2idx)\n",
    "test_sentences= pad_input(test_sentences, seq_len)\n",
    "test_data= TensorDataset(torch.from_numpy(test_sentences))\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "Preds=np.array([],dtype=int)\n",
    "for inputs in test_loader:\n",
    "    inputs= inputs[0].to(device)\n",
    "    model.init_hidden(inputs.size(0))\n",
    "    with torch.no_grad():\n",
    "        output = model(inputs)\n",
    "        _,pred=torch.max(output, 1)\n",
    "    Preds=np.append(Preds,pred.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss 3.1934  Acc : 35.42%  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.193401919470893"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(best_w)\n",
    "Train_model(model,criterion,optimizer,val_loader,device,mode='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "Preds=np.array([],dtype=int)\n",
    "for inputs in test_loader:\n",
    "    inputs= inputs[0].to(device)\n",
    "    model.init_hidden(inputs.size(0))\n",
    "    with torch.no_grad():\n",
    "        output = model(inputs)\n",
    "        _,pred=torch.max(output, 1)\n",
    "    Preds=np.append(Preds,pred.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 - innovate !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('data')"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_TO_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget  -P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_file = r'C:\\Users\\HP\\Desktop\\MVA cours\\NLP\\nlp_project\\word2vec\\glove.840B.300d.txt'\n",
    "tmp_file = get_tmpfile(r'C:\\Users\\HP\\Desktop\\MVA cours\\NLP\\nlp_project\\word2vec\\word2vec.txt')\n",
    "word2vec = glove2word2vec(glove_file, tmp_file)\n",
    "glove = KeyedVectors.load_word2vec_format(tmp_file)\n",
    "embed_dim=glove.vectors.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_s,y_train=create_dataset(train_filepath)\n",
    "val_s,y_val=create_dataset(dev_filepath)\n",
    "test_s=create_dataset(test_filepath,test=True)\n",
    "\n",
    "words = Counter() #Dictionary that will map a word to the number of times it appeared in all the training sentences\n",
    "train_s=Tokenize(train_s,words)\n",
    "val_s=Tokenize(val_s)\n",
    "test_s=Tokenize(test_s)\n",
    "\n",
    "# Adding padding and unknown to our vocabulary so that they will be assigned an index\n",
    "extra=['_PAD','_UNK']\n",
    "\n",
    "words = extra + list(words)\n",
    "\n",
    "#Creating word2idx for Pretrained Glove EMbeddings\n",
    "glove.add(extra,[np.zeros(embed_dim),np.random.rand(embed_dim)*2-1])\n",
    "\n",
    "# Dictionaries to store the word to index mappings and vice versa\n",
    "word2idx={}\n",
    "for word in words:\n",
    "    if word in glove: \n",
    "        word2idx[word] =glove.vocab[word].index\n",
    "    else:\n",
    "        word2idx[word] =glove.vocab['_UNK'].index\n",
    "        \n",
    "train_sentences=one_hot(train_s,word2idx,unknown_index=glove.vocab['_UNK'].index)\n",
    "val_sentences=one_hot(val_s,word2idx,unknown_index=glove.vocab['_UNK'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 52 #Max length on the set \n",
    "train_sentences = pad_input(train_sentences, seq_len,pad_idx=glove.vocab['_PAD'].index)\n",
    "val_sentences= pad_input(val_sentences, seq_len,pad_idx=glove.vocab['_PAD'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(np.array(y_train)))\n",
    "val_data= TensorDataset(torch.from_numpy(val_sentences), torch.from_numpy(np.array(y_val)))\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.46 GiB (GPU 0; 4.00 GiB total capacity; 1.34 GiB already allocated; 1.58 GiB free; 3.21 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-164-854c65c8f1ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglove\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.46 GiB (GPU 0; 4.00 GiB total capacity; 1.34 GiB already allocated; 1.58 GiB free; 3.21 MiB cached)"
     ]
    }
   ],
   "source": [
    "weights = torch.FloatTensor(glove.vectors).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, output_size, hidden_dim, n_layers,weights, drop_prob=0.5):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding =nn.Embedding.from_pretrained(weights)\n",
    "        self.lstm = nn.LSTM(50, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.fc = nn.Sequential(nn.Linear(hidden_dim, output_size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.long()\n",
    "        \n",
    "        embeds = self.embedding(x)\n",
    "        #print(embeds.shape)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        lstm_out = lstm_out.contiguous().view( -1,self.hidden_dim)\n",
    "        #print(lstm_out.shape)\n",
    "        out = self.fc(lstm_out)\n",
    "        out=out.reshape(x.size(0),-1,self.output_size)\n",
    "        return out[:,-1]\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        self.hidden=hidden\n",
    "        return \n",
    "output_size = 5\n",
    "hidden_dim = 30\n",
    "n_layers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(output_size, hidden_dim, n_layers,weights)\n",
    "model.to(device)\n",
    "print(model)\n",
    "lr=0.01\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embedding.weight.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=30\n",
    "acc_t=[]\n",
    "acc_v=[]\n",
    "best_acc=0\n",
    "for epoch in range(epochs):\n",
    "  print('epoch: ',epoch)\n",
    "  acc_t+=[Train_model(model,criterion,optimizer,train_loader,device,mode='train')]\n",
    "  acc_v+=[Train_model(model,criterion,optimizer,val_loader,device,mode='val')]\n",
    "  if acc_v[-1]>best_acc:\n",
    "        best_acc=acc_v[-1]\n",
    "        best_w=copy.deepcopy(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "train Loss 1.1387  Acc : 50.33%  \n",
      "val Loss 1.3957  Acc : 41.05%  \n",
      "epoch:  1\n",
      "train Loss 1.1193  Acc : 51.53%  \n",
      "val Loss 1.4049  Acc : 41.05%  \n",
      "epoch:  2\n",
      "train Loss 1.1104  Acc : 51.46%  \n",
      "val Loss 1.4077  Acc : 41.14%  \n",
      "epoch:  3\n",
      "train Loss 1.0962  Acc : 52.61%  \n",
      "val Loss 1.4207  Acc : 41.24%  \n",
      "epoch:  4\n",
      "train Loss 1.0804  Acc : 53.35%  \n",
      "val Loss 1.3983  Acc : 41.05%  \n",
      "epoch:  5\n",
      "train Loss 1.0723  Acc : 53.21%  \n",
      "val Loss 1.4102  Acc : 41.24%  \n",
      "epoch:  6\n",
      "train Loss 1.0567  Acc : 54.00%  \n",
      "val Loss 1.4094  Acc : 41.51%  \n",
      "epoch:  7\n",
      "train Loss 1.0479  Acc : 55.18%  \n",
      "val Loss 1.4108  Acc : 41.24%  \n",
      "epoch:  8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-156-6dac08c96d59>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m   \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'epoch: '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m   \u001b[0mLosses_t\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m   \u001b[0mLosses_v\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-149-ecfcd6048f19>\u001b[0m in \u001b[0;36mTrain_model\u001b[1;34m(model, criterion, optimizer, loader, device, scheduler, mode)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m     \u001b[1;31m#Preds=np.append(Preds,pred.cpu().long().numpy())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;31m#Actual=np.append(Actual,labels.cpu().long().numpy())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.embedding.weight.requires_grad=True\n",
    "epochs=10\n",
    "Losses_v=[]\n",
    "Losses_t=[]\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)  \n",
    "for epoch in range(epochs):\n",
    "  print('epoch: ',epoch)\n",
    "  Losses_t+=[Train_model(model,criterion,optimizer,train_loader,device,mode='train')]\n",
    "  Losses_v+=[Train_model(model,criterion,optimizer,val_loader,device,mode='val')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 - Open question: find a model that is better on your dev set\n",
    "#     (e.g: use a 1D ConvNet, use a better classifier, pretrain your lookup tables ..)\n",
    "#     you will get point if the results on the test set are better: be careful of not overfitting your dev set too much..\n",
    "#     Attach the output file \"XXX_XXX_y_test_sst.txt\" to your deliverable.\n",
    "plt.plot(acc_t,label='Train Accuracy')\n",
    "plt.plot(acc_v,label='val Accuracy')\n",
    "plt.title('Accuracy Evolution')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
